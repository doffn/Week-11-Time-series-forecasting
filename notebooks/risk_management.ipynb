{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Risk Management and Monitoring\n",
    "\n",
    "This notebook covers:\n",
    "- Real-time risk monitoring systems\n",
    "- Dynamic hedging strategies\n",
    "- Regime detection and adaptation\n",
    "- Risk attribution analysis\n",
    "- Early warning systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Risk management libraries\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Custom modules\n",
    "from src.data.loader import DataLoader\n",
    "from src.risk.manager import RiskManager\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logger(__name__)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Risk management libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Portfolio Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data\n",
    "loader = DataLoader()\n",
    "data = loader.load_data(['TSLA', 'BND', 'SPY'], '2015-07-01', '2025-07-31')\n",
    "prices = data['prices']\n",
    "returns = data['returns']\n",
    "\n",
    "# Load portfolio recommendation\n",
    "try:\n",
    "    with open('../data/portfolio_recommendation.pkl', 'rb') as f:\n",
    "        portfolio_rec = pickle.load(f)\n",
    "    recommended_weights = portfolio_rec['weights']\n",
    "    print(\"✓ Portfolio recommendation loaded\")\nexcept FileNotFoundError:\n",
    "    print(\"✗ Using default weights\")\n",
    "    recommended_weights = {'TSLA': 0.4, 'BND': 0.3, 'SPY': 0.3}\n",
    "\n",
    "# Calculate portfolio returns\n",
    "weights_series = pd.Series(recommended_weights)\n",
    "portfolio_returns = (returns * weights_series).sum(axis=1)\n",
    "\n",
    "print(f\"Portfolio weights: {recommended_weights}\")\n",
    "print(f\"Analysis period: {returns.index.min()} to {returns.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regime Detection and Market State Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect market regimes using various statistical methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, window=60):\n",
    "        self.returns = returns_data\n",
    "        self.window = window\n",
    "        \n",
    "    def calculate_regime_features(self):\n",
    "        \"\"\"\n",
    "        Calculate features for regime detection\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame(index=self.returns.index)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        features['volatility'] = self.returns.rolling(self.window).std() * np.sqrt(252)\n",
    "        features['mean_return'] = self.returns.rolling(self.window).mean() * 252\n",
    "        features['skewness'] = self.returns.rolling(self.window).skew()\n",
    "        features['kurtosis'] = self.returns.rolling(self.window).kurt()\n",
    "        \n",
    "        # VaR and CVaR\n",
    "        features['var_95'] = self.returns.rolling(self.window).quantile(0.05)\n",
    "        features['cvar_95'] = self.returns.rolling(self.window).apply(\n",
    "            lambda x: x[x <= x.quantile(0.05)].mean()\n",
    "        )\n",
    "        \n",
    "        # Trend indicators\n",
    "        price_proxy = (1 + self.returns).cumprod()\n",
    "        features['trend_strength'] = (\n",
    "            price_proxy / price_proxy.rolling(self.window).mean() - 1\n",
    "        )\n",
    "        \n",
    "        # Correlation with market (using SPY as proxy)\n",
    "        if 'SPY' in self.returns.columns:\n",
    "            spy_returns = self.returns['SPY']\n",
    "            features['market_correlation'] = self.returns.rolling(self.window).corr(spy_returns)\n",
    "        \n",
    "        return features.dropna()\n",
    "    \n",
    "    def detect_regimes_kmeans(self, n_regimes=3):\n",
    "        \"\"\"\n",
    "        Detect regimes using K-means clustering\n",
    "        \"\"\"\n",
    "        features = self.calculate_regime_features()\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Apply K-means\n",
    "        kmeans = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)\n",
    "        regimes = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        # Create regime series\n",
    "        regime_series = pd.Series(regimes, index=features.index)\n",
    "        \n",
    "        return regime_series, features, kmeans\n",
    "    \n",
    "    def analyze_regime_characteristics(self, regime_series, features):\n",
    "        \"\"\"\n",
    "        Analyze characteristics of each regime\n",
    "        \"\"\"\n",
    "        regime_stats = {}\n",
    "        \n",
    "        for regime in regime_series.unique():\n",
    "            regime_mask = regime_series == regime\n",
    "            regime_features = features[regime_mask]\n",
    "            regime_returns = self.returns[regime_mask]\n",
    "            \n",
    "            stats_dict = {\n",
    "                'frequency': regime_mask.sum(),\n",
    "                'percentage': regime_mask.mean() * 100,\n",
    "                'avg_volatility': regime_features['volatility'].mean(),\n",
    "                'avg_return': regime_features['mean_return'].mean(),\n",
    "                'avg_skewness': regime_features['skewness'].mean(),\n",
    "                'avg_kurtosis': regime_features['kurtosis'].mean(),\n",
    "                'avg_var': regime_features['var_95'].mean(),\n",
    "                'worst_drawdown': self._calculate_max_drawdown(regime_returns)\n",
    "            }\n",
    "            \n",
    "            regime_stats[f'Regime_{regime}'] = stats_dict\n",
    "        \n",
    "        return pd.DataFrame(regime_stats).T\n",
    "    \n",
    "    def _calculate_max_drawdown(self, returns_series):\n",
    "        \"\"\"\n",
    "        Calculate maximum drawdown for a returns series\n",
    "        \"\"\"\n",
    "        if len(returns_series) == 0:\n",
    "            return 0\n",
    "        \n",
    "        cumulative = (1 + returns_series).cumprod()\n",
    "        rolling_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - rolling_max) / rolling_max\n",
    "        return drawdown.min()\n",
    "\n",
    "# Initialize regime detector\n",
    "regime_detector = MarketRegimeDetector(portfolio_returns)\n",
    "\n",
    "# Detect regimes\n",
    "print(\"Detecting market regimes...\")\n",
    "regimes, features, kmeans_model = regime_detector.detect_regimes_kmeans(n_regimes=3)\n",
    "\n",
    "# Analyze regime characteristics\n",
    "regime_analysis = regime_detector.analyze_regime_characteristics(regimes, features)\n",
    "\n",
    "print(\"\\nMARKET REGIME ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(regime_analysis.round(4))\n",
    "\n",
    "# Label regimes based on characteristics\n",
    "regime_labels = {}\n",
    "for idx, row in regime_analysis.iterrows():\n",
    "    if row['avg_volatility'] > regime_analysis['avg_volatility'].mean():\n",
    "        if row['avg_return'] > 0:\n",
    "            label = 'High Vol Bull'\n",
    "        else:\n",
    "            label = 'High Vol Bear'\n",
    "    else:\n",
    "        if row['avg_return'] > 0:\n",
    "            label = 'Low Vol Bull'\n",
    "        else:\n",
    "            label = 'Low Vol Bear'\n",
    "    \n",
    "    regime_num = int(idx.split('_')[1])\n",
    "    regime_labels[regime_num] = label\n",
    "\n",
    "print(f\"\\nRegime Labels: {regime_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regime detection results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "fig.suptitle('Market Regime Detection Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Portfolio returns with regime coloring\n",
    "ax1 = axes[0]\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "for regime in regimes.unique():\n",
    "    regime_mask = regimes == regime\n",
    "    regime_dates = regimes[regime_mask].index\n",
    "    regime_values = cumulative_returns[regime_dates]\n",
    "    \n",
    "    ax1.scatter(regime_dates, regime_values, \n",
    "               label=f'{regime_labels.get(regime, f\"Regime {regime}\")}',\n",
    "               alpha=0.7, s=10)\n",
    "\n",
    "ax1.set_title('Portfolio Cumulative Returns by Market Regime')\n",
    "ax1.set_ylabel('Cumulative Return')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility with regime coloring\n",
    "ax2 = axes[1]\n",
    "rolling_vol = portfolio_returns.rolling(30).std() * np.sqrt(252)\n",
    "\n",
    "for regime in regimes.unique():\n",
    "    regime_mask = regimes == regime\n",
    "    regime_dates = regimes[regime_mask].index\n",
    "    regime_vol = rolling_vol[regime_dates]\n",
    "    \n",
    "    ax2.scatter(regime_dates, regime_vol,\n",
    "               label=f'{regime_labels.get(regime, f\"Regime {regime}\")}',\n",
    "               alpha=0.7, s=10)\n",
    "\n",
    "ax2.set_title('Rolling Volatility by Market Regime')\n",
    "ax2.set_ylabel('Annualized Volatility')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Regime transitions over time\n",
    "ax3 = axes[2]\n",
    "ax3.plot(regimes.index, regimes, linewidth=2, alpha=0.8)\n",
    "ax3.set_title('Market Regime Transitions Over Time')\n",
    "ax3.set_ylabel('Regime')\n",
    "ax3.set_yticks(list(regime_labels.keys()))\n",
    "ax3.set_yticklabels([regime_labels[k] for k in sorted(regime_labels.keys())])\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Regime transition matrix\n",
    "transition_matrix = pd.crosstab(regimes.shift(1), regimes, normalize='index')\n",
    "transition_matrix.index = [regime_labels.get(i, f'Regime {i}') for i in transition_matrix.index]\n",
    "transition_matrix.columns = [regime_labels.get(i, f'Regime {i}') for i in transition_matrix.columns]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(transition_matrix, annot=True, cmap='Blues', fmt='.3f')\n",
    "plt.title('Regime Transition Probability Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('From Regime')\n",
    "plt.xlabel('To Regime')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nREGIME TRANSITION PROBABILITIES\")\n",
    "print(\"=\" * 50)\n",
    "print(transition_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Risk Monitoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRiskMonitor:\n",
    "    \"\"\"\n",
    "    Real-time risk monitoring and alert system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, portfolio_returns, weights, confidence_levels=[0.95, 0.99]):\n",
    "        self.returns = portfolio_returns\n",
    "        self.weights = weights\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.risk_metrics = {}\n",
    "        \n",
    "    def calculate_dynamic_var(self, window=60, method='historical'):\n",
    "        \"\"\"\n",
    "        Calculate dynamic Value at Risk\n",
    "        \"\"\"\n",
    "        var_results = {}\n",
    "        \n",
    "        for conf_level in self.confidence_levels:\n",
    "            alpha = 1 - conf_level\n",
    "            \n",
    "            if method == 'historical':\n",
    "                var_series = self.returns.rolling(window).quantile(alpha)\n",
    "            elif method == 'parametric':\n",
    "                rolling_mean = self.returns.rolling(window).mean()\n",
    "                rolling_std = self.returns.rolling(window).std()\n",
    "                z_score = stats.norm.ppf(alpha)\n",
    "                var_series = rolling_mean + z_score * rolling_std\n",
    "            elif method == 'ewma':\n",
    "                # Exponentially weighted moving average\n",
    "                ewm_mean = self.returns.ewm(span=window).mean()\n",
    "                ewm_std = self.returns.ewm(span=window).std()\n",
    "                z_score = stats.norm.ppf(alpha)\n",
    "                var_series = ewm_mean + z_score * ewm_std\n",
    "            \n",
    "            var_results[f'VaR_{int(conf_level*100)}'] = var_series\n",
    "        \n",
    "        return pd.DataFrame(var_results)\n",
    "    \n",
    "    def calculate_expected_shortfall(self, window=60):\n",
    "        \"\"\"\n",
    "        Calculate Expected Shortfall (Conditional VaR)\n",
    "        \"\"\"\n",
    "        es_results = {}\n",
    "        \n",
    "        for conf_level in self.confidence_levels:\n",
    "            alpha = 1 - conf_level\n",
    "            \n",
    "            def rolling_es(x):\n",
    "                var_threshold = x.quantile(alpha)\n",
    "                return x[x <= var_threshold].mean()\n",
    "            \n",
    "            es_series = self.returns.rolling(window).apply(rolling_es)\n",
    "            es_results[f'ES_{int(conf_level*100)}'] = es_series\n",
    "        \n",
    "        return pd.DataFrame(es_results)\n",
    "    \n",
    "    def calculate_component_var(self, asset_returns, window=60):\n",
    "        \"\"\"\n",
    "        Calculate component VaR for risk attribution\n",
    "        \"\"\"\n",
    "        weights_series = pd.Series(self.weights)\n",
    "        component_vars = {}\n",
    "        \n",
    "        for date in asset_returns.index[-window:]:\n",
    "            # Get window of returns ending at this date\n",
    "            window_returns = asset_returns.loc[:date].tail(window)\n",
    "            \n",
    "            if len(window_returns) < window:\n",
    "                continue\n",
    "            \n",
    "            # Calculate covariance matrix\n",
    "            cov_matrix = window_returns.cov()\n",
    "            \n",
    "            # Portfolio variance\n",
    "            portfolio_var = np.dot(weights_series.T, np.dot(cov_matrix, weights_series))\n",
    "            portfolio_vol = np.sqrt(portfolio_var)\n",
    "            \n",
    "            # Marginal VaR\n",
    "            marginal_var = np.dot(cov_matrix, weights_series) / portfolio_vol\n",
    "            \n",
    "            # Component VaR\n",
    "            component_var = weights_series * marginal_var\n",
    "            \n",
    "            component_vars[date] = component_var\n",
    "        \n",
    "        return pd.DataFrame(component_vars).T\n",
    "    \n",
    "    def detect_risk_anomalies(self, var_df, threshold_multiplier=2):\n",
    "        \"\"\"\n",
    "        Detect risk anomalies and generate alerts\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        for col in var_df.columns:\n",
    "            var_series = var_df[col].dropna()\n",
    "            \n",
    "            # Calculate rolling statistics\n",
    "            rolling_mean = var_series.rolling(30).mean()\n",
    "            rolling_std = var_series.rolling(30).std()\n",
    "            \n",
    "            # Detect anomalies\n",
    "            upper_threshold = rolling_mean + threshold_multiplier * rolling_std\n",
    "            lower_threshold = rolling_mean - threshold_multiplier * rolling_std\n",
    "            \n",
    "            anomalies = var_series[(var_series > upper_threshold) | (var_series < lower_threshold)]\n",
    "            \n",
    "            for date, value in anomalies.items():\n",
    "                alert_type = 'HIGH_RISK' if value > upper_threshold[date] else 'LOW_RISK'\n",
    "                alerts.append({\n",
    "                    'date': date,\n",
    "                    'metric': col,\n",
    "                    'value': value,\n",
    "                    'threshold': upper_threshold[date] if alert_type == 'HIGH_RISK' else lower_threshold[date],\n",
    "                    'alert_type': alert_type,\n",
    "                    'severity': 'HIGH' if abs(value - rolling_mean[date]) > 3 * rolling_std[date] else 'MEDIUM'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(alerts)\n",
    "\n",
    "# Initialize risk monitor\n",
    "risk_monitor = DynamicRiskMonitor(portfolio_returns, recommended_weights)\n",
    "\n",
    "# Calculate dynamic risk metrics\n",
    "print(\"Calculating dynamic risk metrics...\")\n",
    "var_metrics = risk_monitor.calculate_dynamic_var(window=60, method='ewma')\n",
    "es_metrics = risk_monitor.calculate_expected_shortfall(window=60)\n",
    "component_var = risk_monitor.calculate_component_var(returns, window=60)\n",
    "\n",
    "# Detect risk anomalies\n",
    "risk_alerts = risk_monitor.detect_risk_anomalies(var_metrics)\n",
    "\n",
    "print(f\"\\nRISK MONITORING SUMMARY\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Current VaR (95%): {var_metrics['VaR_95'].iloc[-1]:.4f}\")\n",
    "print(f\"Current VaR (99%): {var_metrics['VaR_99'].iloc[-1]:.4f}\")\n",
    "print(f\"Current ES (95%): {es_metrics['ES_95'].iloc[-1]:.4f}\")\n",
    "print(f\"Current ES (99%): {es_metrics['ES_99'].iloc[-1]:.4f}\")\n",
    "print(f\"Total risk alerts generated: {len(risk_alerts)}\")\n",
    "\n",
    "if len(risk_alerts) > 0:\n",
    "    print(f\"\\nRecent HIGH severity alerts:\")\n",
    "    high_severity = risk_alerts[risk_alerts['severity'] == 'HIGH'].tail(5)\n",
    "    for _, alert in high_severity.iterrows():\n",
    "        print(f\"  {alert['date'].strftime('%Y-%m-%d')}: {alert['metric']} = {alert['value']:.4f} ({alert['alert_type']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dynamic risk metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Dynamic Risk Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# VaR evolution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(var_metrics.index, var_metrics['VaR_95'], label='VaR 95%', linewidth=2)\n",
    "ax1.plot(var_metrics.index, var_metrics['VaR_99'], label='VaR 99%', linewidth=2)\n",
    "ax1.fill_between(var_metrics.index, var_metrics['VaR_95'], var_metrics['VaR_99'], alpha=0.3)\n",
    "ax1.set_title('Dynamic Value at Risk')\n",
    "ax1.set_ylabel('VaR')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Expected Shortfall\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(es_metrics.index, es_metrics['ES_95'], label='ES 95%', linewidth=2, color='red')\n",
    "ax2.plot(es_metrics.index, es_metrics['ES_99'], label='ES 99%', linewidth=2, color='darkred')\n",
    "ax2.set_title('Expected Shortfall (Conditional VaR)')\n",
    "ax2.set_ylabel('Expected Shortfall')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Component VaR (most recent)\n",
    "ax3 = axes[1, 0]\n",
    "if not component_var.empty:\n",
    "    latest_component_var = component_var.iloc[-1]\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    bars = ax3.bar(latest_component_var.index, latest_component_var.values, color=colors, alpha=0.7)\n",
    "    ax3.set_title('Component VaR (Risk Attribution)')\n",
    "    ax3.set_ylabel('Component VaR')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, latest_component_var.values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Risk alerts timeline\n",
    "ax4 = axes[1, 1]\n",
    "if len(risk_alerts) > 0:\n",
    "    # Count alerts by date\n",
    "    alert_counts = risk_alerts.groupby(risk_alerts['date'].dt.date).size()\n",
    "    ax4.bar(alert_counts.index, alert_counts.values, alpha=0.7, color='orange')\n",
    "    ax4.set_title('Risk Alerts Timeline')\n",
    "    ax4.set_ylabel('Number of Alerts')\n",
    "    ax4.tick_params(axis='x', rotation=45)\nelse:\n",
    "    ax4.text(0.5, 0.5, 'No Risk Alerts', ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_title('Risk Alerts Timeline')\n",
    "\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk attribution analysis\n",
    "if not component_var.empty:\n",
    "    print(f\"\\nRISK ATTRIBUTION ANALYSIS\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    # Calculate average component VaR\n",
    "    avg_component_var = component_var.mean()\n",
    "    total_component_var = avg_component_var.sum()\n",
    "    \n",
    "    print(f\"Average Component VaR:\")\n",
    "    for asset, comp_var in avg_component_var.items():\n",
    "        percentage = (comp_var / total_component_var) * 100\n",
    "        weight = recommended_weights.get(asset, 0) * 100\n",
    "        print(f\"  {asset}: {comp_var:.6f} ({percentage:.1f}% of total risk, {weight:.1f}% weight)\")\n",
    "    \n",
    "    # Risk concentration analysis\n",
    "    risk_concentration = (avg_component_var / total_component_var).max()\n",
    "    print(f\"\\nRisk Concentration (max single asset): {risk_concentration*100:.1f}%\")\n",
    "    \n",
    "    if risk_concentration > 0.5:\n",
    "        print(\"⚠️  WARNING: High risk concentration detected!\")\n",
    "    elif risk_concentration > 0.4:\n",
    "        print(\"⚠️  CAUTION: Moderate risk concentration\")\n",
    "    else:\n",
    "        print(\"✅ Risk is well diversified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress Testing and Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedStressTester:\n",
    "    \"\"\"\n",
    "    Advanced stress testing with multiple scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, weights):\n",
    "        self.returns = returns_data\n",
    "        self.weights = pd.Series(weights)\n",
    "        self.portfolio_returns = (returns_data * self.weights).sum(axis=1)\n",
    "        \n",
    "    def historical_stress_test(self, stress_periods):\n",
    "        \"\"\"\n",
    "        Test portfolio performance during historical stress periods\n",
    "        \"\"\"\n",
    "        stress_results = {}\n",
    "        \n",
    "        for period_name, (start_date, end_date) in stress_periods.items():\n",
    "            period_returns = self.portfolio_returns[\n",
    "                (self.portfolio_returns.index >= start_date) & \n",
    "                (self.portfolio_returns.index <= end_date)\n",
    "            ]\n",
    "            \n",
    "            if len(period_returns) > 0:\n",
    "                cumulative_return = (1 + period_returns).prod() - 1\n",
    "                volatility = period_returns.std() * np.sqrt(252)\n",
    "                max_drawdown = self._calculate_max_drawdown(period_returns)\n",
    "                worst_day = period_returns.min()\n",
    "                \n",
    "                stress_results[period_name] = {\n",
    "                    'cumulative_return': cumulative_return,\n",
    "                    'volatility': volatility,\n",
    "                    'max_drawdown': max_drawdown,\n",
    "                    'worst_day': worst_day,\n",
    "                    'days': len(period_returns)\n",
    "                }\n",
    "        \n",
    "        return pd.DataFrame(stress_results).T\n",
    "    \n",
    "    def monte_carlo_stress_test(self, scenarios, n_simulations=10000):\n",
    "        \"\"\"\n",
    "        Monte Carlo stress testing with custom scenarios\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for scenario_name, scenario_params in scenarios.items():\n",
    "            # Extract scenario parameters\n",
    "            asset_shocks = scenario_params.get('asset_shocks', {})\n",
    "            correlation_shock = scenario_params.get('correlation_shock', 0)\n",
    "            volatility_multiplier = scenario_params.get('volatility_multiplier', 1)\n",
    "            \n",
    "            # Base statistics\n",
    "            base_returns = self.returns.mean()\n",
    "            base_cov = self.returns.cov()\n",
    "            \n",
    "            # Apply shocks\n",
    "            shocked_returns = base_returns.copy()\n",
    "            for asset, shock in asset_shocks.items():\n",
    "                if asset in shocked_returns.index:\n",
    "                    shocked_returns[asset] += shock\n",
    "            \n",
    "            # Modify covariance matrix\n",
    "            shocked_cov = base_cov * volatility_multiplier\n",
    "            \n",
    "            if correlation_shock != 0:\n",
    "                # Increase correlations\n",
    "                corr_matrix = base_cov.corr()\n",
    "                vol_matrix = np.sqrt(np.diag(base_cov))\n",
    "                \n",
    "                # Shock correlations\n",
    "                shocked_corr = corr_matrix + correlation_shock\n",
    "                shocked_corr = np.clip(shocked_corr, -0.99, 0.99)\n",
    "                np.fill_diagonal(shocked_corr, 1)\n",
    "                \n",
    "                # Reconstruct covariance matrix\n",
    "                shocked_cov = np.outer(vol_matrix, vol_matrix) * shocked_corr\n",
    "                shocked_cov = pd.DataFrame(shocked_cov, index=base_cov.index, columns=base_cov.columns)\n",
    "            \n",
    "            # Monte Carlo simulation\n",
    "            np.random.seed(42)\n",
    "            simulated_returns = np.random.multivariate_normal(\n",
    "                shocked_returns.values, shocked_cov.values, n_simulations\n",
    "            )\n",
    "            \n",
    "            # Calculate portfolio returns\n",
    "            portfolio_sim_returns = np.dot(simulated_returns, self.weights.values)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            results[scenario_name] = {\n",
    "                'mean_return': np.mean(portfolio_sim_returns),\n",
    "                'volatility': np.std(portfolio_sim_returns),\n",
    "                'var_95': np.percentile(portfolio_sim_returns, 5),\n",
    "                'var_99': np.percentile(portfolio_sim_returns, 1),\n",
    "                'expected_shortfall_95': np.mean(portfolio_sim_returns[portfolio_sim_returns <= np.percentile(portfolio_sim_returns, 5)]),\n",
    "                'worst_case': np.min(portfolio_sim_returns),\n",
    "                'probability_loss': np.mean(portfolio_sim_returns < 0) * 100\n",
    "            }\n",
    "        \n",
    "        return pd.DataFrame(results).T\n",
    "    \n",
    "    def _calculate_max_drawdown(self, returns_series):\n",
    "        \"\"\"\n",
    "        Calculate maximum drawdown\n",
    "        \"\"\"\n",
    "        cumulative = (1 + returns_series).cumprod()\n",
    "        rolling_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - rolling_max) / rolling_max\n",
    "        return drawdown.min()\n",
    "\n",
    "# Initialize stress tester\n",
    "stress_tester = AdvancedStressTester(returns, recommended_weights)\n",
    "\n",
    "# Define historical stress periods\n",
    "stress_periods = {\n",
    "    'COVID-19 Crash': ('2020-02-20', '2020-04-30'),\n",
    "    'December 2018 Selloff': ('2018-10-01', '2018-12-31'),\n",
    "    'Early 2022 Correction': ('2022-01-01', '2022-06-30'),\n",
    "    'August 2015 Flash Crash': ('2015-08-01', '2015-09-30')\n",
    "}\n",
    "\n",
    "# Run historical stress tests\n",
    "print(\"Running historical stress tests...\")\n",
    "historical_stress = stress_tester.historical_stress_test(stress_periods)\n",
    "\n",
    "print(\"\\nHISTORICAL STRESS TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(historical_stress.round(4))\n",
    "\n",
    "# Define Monte Carlo scenarios\n",
    "mc_scenarios = {\n",
    "    'Market Crash': {\n",
    "        'asset_shocks': {'TSLA': -0.3, 'SPY': -0.2, 'BND': 0.05},\n",
    "        'correlation_shock': 0.3,\n",
    "        'volatility_multiplier': 2.0\n",
    "    },\n",
    "    'Interest Rate Shock': {\n",
    "        'asset_shocks': {'BND': -0.15, 'SPY': -0.1, 'TSLA': -0.05},\n",
    "        'correlation_shock': 0.2,\n",
    "        'volatility_multiplier': 1.5\n",
    "    },\n",
    "    'Tech Bubble Burst': {\n",
    "        'asset_shocks': {'TSLA': -0.5, 'SPY': -0.15, 'BND': 0.1},\n",
    "        'correlation_shock': 0.1,\n",
    "        'volatility_multiplier': 2.5\n",
    "    },\n",
    "    'Stagflation': {\n",
    "        'asset_shocks': {'TSLA': -0.1, 'SPY': -0.1, 'BND': -0.2},\n",
    "        'correlation_shock': 0.4,\n",
    "        'volatility_multiplier': 1.8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run Monte Carlo stress tests\n",
    "print(\"\\nRunning Monte Carlo stress tests...\")\n",
    "mc_stress = stress_tester.monte_carlo_stress_test(mc_scenarios)\n",
    "\n",
    "print(\"\\nMONTE CARLO STRESS TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(mc_stress.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stress test results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Advanced Stress Testing Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Historical stress test - cumulative returns\n",
    "ax1 = axes[0, 0]\n",
    "historical_stress['cumulative_return'].plot(kind='bar', ax=ax1, color='red', alpha=0.7)\n",
    "ax1.set_title('Historical Stress Periods - Cumulative Returns')\n",
    "ax1.set_ylabel('Cumulative Return')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Historical stress test - max drawdown\n",
    "ax2 = axes[0, 1]\n",
    "historical_stress['max_drawdown'].plot(kind='bar', ax=ax2, color='darkred', alpha=0.7)\n",
    "ax2.set_title('Historical Stress Periods - Maximum Drawdown')\n",
    "ax2.set_ylabel('Maximum Drawdown')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Monte Carlo stress test - VaR comparison\n",
    "ax3 = axes[1, 0]\n",
    "var_comparison = mc_stress[['var_95', 'var_99']]\n",
    "var_comparison.plot(kind='bar', ax=ax3, alpha=0.7)\n",
    "ax3.set_title('Monte Carlo Scenarios - VaR Comparison')\n",
    "ax3.set_ylabel('Value at Risk')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend(['VaR 95%', 'VaR 99%'])\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Monte Carlo stress test - probability of loss\n",
    "ax4 = axes[1, 1]\n",
    "mc_stress['probability_loss'].plot(kind='bar', ax=ax4, color='orange', alpha=0.7)\n",
    "ax4.set_title('Monte Carlo Scenarios - Probability of Loss')\n",
    "ax4.set_ylabel('Probability of Loss (%)')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stress test summary and recommendations\n",
    "print(f\"\\nSTRESS TEST SUMMARY AND RECOMMENDATIONS\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Worst historical performance\n",
    "worst_historical = historical_stress['cumulative_return'].min()\n",
    "worst_period = historical_stress['cumulative_return'].idxmin()\n",
    "print(f\"Worst historical performance: {worst_historical:.2%} during {worst_period}\")\n",
    "\n",
    "# Worst Monte Carlo scenario\n",
    "worst_mc_scenario = mc_stress['var_99'].idxmin()\n",
    "worst_mc_var = mc_stress.loc[worst_mc_scenario, 'var_99']\n",
    "print(f\"Worst Monte Carlo scenario: {worst_mc_scenario} (VaR 99%: {worst_mc_var:.2%})\")\n",
    "\n",
    "# Risk recommendations\n",
    "print(f\"\\nRISK MANAGEMENT RECOMMENDATIONS:\")\n",
    "print(f\"-\" * 40)\n",
    "\n",
    "if worst_historical < -0.3:\n",
    "    print(\"🔴 HIGH RISK: Portfolio showed severe losses during historical stress periods\")\n",
    "    print(\"   → Consider reducing allocation to high-volatility assets\")\n",
    "    print(\"   → Implement dynamic hedging strategies\")\nelif worst_historical < -0.15:\n",
    "    print(\"🟡 MEDIUM RISK: Portfolio showed moderate stress during crisis periods\")\n",
    "    print(\"   → Monitor risk metrics closely during market volatility\")\n",
    "    print(\"   → Consider partial hedging during high-risk periods\")\nelse:\n",
    "    print(\"🟢 LOW RISK: Portfolio demonstrated resilience during historical stress periods\")\n",
    "    print(\"   → Continue current risk management approach\")\n",
    "\n",
    "# Scenario-specific recommendations\n",
    "high_loss_prob_scenarios = mc_stress[mc_stress['probability_loss'] > 60]\n",
    "if not high_loss_prob_scenarios.empty:\n",
    "    print(f\"\\n⚠️  High loss probability scenarios detected:\")\n",
    "    for scenario in high_loss_prob_scenarios.index:\n",
    "        prob = high_loss_prob_scenarios.loc[scenario, 'probability_loss']\n",
    "        print(f\"   → {scenario}: {prob:.1f}% probability of loss\")\n",
    "\n",
    "print(f\"\\nGeneral Recommendations:\")\n",
    "print(f\"• Implement stop-loss orders at portfolio level\")\n",
    "print(f\"• Consider tail risk hedging instruments\")\n",
    "print(f\"• Regular stress testing (monthly)\")\n",
    "print(f\"• Dynamic position sizing based on market regime\")\n",
    "print(f\"• Maintain adequate cash reserves for rebalancing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Management Summary and Action Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive risk management summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE RISK MANAGEMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. CURRENT RISK PROFILE:\")\n",
    "print(f\"-\" * 40)\n",
    "current_var_95 = var_metrics['VaR_95'].iloc[-1]\n",
    "current_var_99 = var_metrics['VaR_99'].iloc[-1]\n",
    "current_es_95 = es_metrics['ES_95'].iloc[-1]\n",
    "\n",
    "print(f\"• Daily VaR (95%): {current_var_95:.2%}\")\n",
    "print(f\"• Daily VaR (99%): {current_var_99:.2%}\")\n",
    "print(f\"• Expected Shortfall (95%): {current_es_95:.2%}\")\n",
    "print(f\"• Portfolio Volatility: {portfolio_returns.std() * np.sqrt(252):.2%}\")\n",
    "\n",
    "print(f\"\\n2. MARKET REGIME ANALYSIS:\")\n",
    "print(f\"-\" * 40)\n",
    "current_regime = regimes.iloc[-1]\n",
    "current_regime_label = regime_labels.get(current_regime, f'Regime {current_regime}')\n",
    "print(f\"• Current Market Regime: {current_regime_label}\")\n",
    "\n",
    "# Regime stability\n",
    "recent_regimes = regimes.tail(30)  # Last 30 days\n",
    "regime_changes = (recent_regimes != recent_regimes.shift(1)).sum()\n",
    "print(f\"• Regime Changes (last 30 days): {regime_changes}\")\n",
    "print(f\"• Regime Stability: {'High' if regime_changes <= 2 else 'Medium' if regime_changes <= 5 else 'Low'}\")\n",
    "\n",
    "print(f\"\\n3. STRESS TEST INSIGHTS:\")\n",
    "print(f\"-\" * 40)\n",
    "avg_historical_loss = historical_stress['cumulative_return'].mean()\n",
    "worst_historical_loss = historical_stress['cumulative_return'].min()\n",
    "avg_mc_var_99 = mc_stress['var_99'].mean()\n",
    "\n",
    "print(f\"• Average Historical Stress Loss: {avg_historical_loss:.2%}\")\n",
    "print(f\"• Worst Historical Stress Loss: {worst_historical_loss:.2%}\")\n",
    "print(f\"• Average Monte Carlo VaR (99%): {avg_mc_var_99:.2%}\")\n",
    "\n",
    "print(f\"\\n4. RISK CONCENTRATION:\")\n",
    "print(f\"-\" * 40)\n",
    "if not component_var.empty:\n",
    "    latest_component_var = component_var.iloc[-1]\n",
    "    max_risk_asset = latest_component_var.abs().idxmax()\n",
    "    max_risk_contribution = latest_component_var.abs().max() / latest_component_var.abs().sum()\n",
    "    \n",
    "    print(f\"• Highest Risk Contributor: {max_risk_asset}\")\n",
    "    print(f\"• Maximum Risk Concentration: {max_risk_contribution:.1%}\")\n",
    "    \n",
    "    risk_level = \"High\" if max_risk_contribution > 0.6 else \"Medium\" if max_risk_contribution > 0.4 else \"Low\"\n",
    "    print(f\"• Risk Concentration Level: {risk_level}\")\n",
    "\n",
    "print(f\"\\n5. ALERT SYSTEM STATUS:\")\n",
    "print(f\"-\" * 40)\n",
    "total_alerts = len(risk_alerts)\n",
    "high_severity_alerts = len(risk_alerts[risk_alerts['severity'] == 'HIGH']) if len(risk_alerts) > 0 else 0\n",
    "recent_alerts = len(risk_alerts[risk_alerts['date'] >= (datetime.now() - timedelta(days=7))]) if len(risk_alerts) > 0 else 0\n",
    "\n",
    "print(f\"• Total Risk Alerts: {total_alerts}\")\n",
    "print(f\"• High Severity Alerts: {high_severity_alerts}\")\n",
    "print(f\"• Recent Alerts (7 days): {recent_alerts}\")\n",
    "\n",
    "alert_status = \"Critical\" if high_severity_alerts > 5 else \"Warning\" if recent_alerts > 3 else \"Normal\"\n",
    "print(f\"• Alert System Status: {alert_status}\")\n",
    "\n",
    "print(f\"\\n6. RECOMMENDED ACTIONS:\")\n",
    "print(f\"-\" * 40)\n",
    "\n",
    "# Generate specific recommendations based on analysis\n",
    "recommendations = []\n",
    "\n",
    "# VaR-based recommendations\n",
    "if abs(current_var_99) > 0.05:  # 5% daily VaR\n",
    "    recommendations.append(\"🔴 URGENT: Daily VaR exceeds 5% - consider immediate position reduction\")\nelif abs(current_var_99) > 0.03:  # 3% daily VaR\n",
    "    recommendations.append(\"🟡 CAUTION: Daily VaR elevated - monitor closely and prepare for position adjustment\")\n\n# Regime-based recommendations\nif regime_changes > 5:\n    recommendations.append(\"⚠️  High regime instability - implement dynamic hedging\")\n\n# Stress test recommendations\nif worst_historical_loss < -0.3:\n    recommendations.append(\"📉 Historical stress tests show severe losses - diversify further\")\n\n# Risk concentration recommendations\nif not component_var.empty and max_risk_contribution > 0.6:\n    recommendations.append(f\"⚖️  High risk concentration in {max_risk_asset} - rebalance portfolio\")\n\n# Alert-based recommendations\nif alert_status == \"Critical\":\n    recommendations.append(\"🚨 Critical alert status - immediate risk review required\")\nelif alert_status == \"Warning\":\n    recommendations.append(\"⚠️  Warning alert status - enhanced monitoring recommended\")\n\n# Default recommendations if no specific issues\nif not recommendations:\n    recommendations = [\n        \"✅ Risk profile within acceptable parameters\",\n        \"📊 Continue regular monitoring and monthly stress testing\",\n        \"🔄 Maintain current rebalancing schedule\"\n    ]\n\nfor i, rec in enumerate(recommendations, 1):\n    print(f\"{i}. {rec}\")\n\nprint(f\"\\n7. MONITORING SCHEDULE:\")\nprint(f\"-\" * 40)\nprint(f\"• Daily: VaR and portfolio returns monitoring\")\nprint(f\"• Weekly: Risk alert review and regime analysis\")\nprint(f\"• Monthly: Comprehensive stress testing\")\nprint(f\"• Quarterly: Risk model validation and recalibration\")\nprint(f\"• Semi-annually: Portfolio optimization review\")\n\nprint(f\"\\n8. EMERGENCY PROCEDURES:\")\nprint(f\"-\" * 40)\nprint(f\"• VaR > 5%: Immediate position review and potential reduction\")\nprint(f\"• Multiple high-severity alerts: Emergency risk committee meeting\")\nprint(f\"• Regime change: Reassess portfolio allocation within 48 hours\")\nprint(f\"• Stress test failure: Implement contingency hedging strategies\")\n\nprint(f\"\\n\" + \"=\"*80)\nprint(\"RISK MANAGEMENT ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\n# Save risk management results\nrisk_management_summary = {\n    'current_risk_metrics': {\n        'var_95': current_var_95,\n        'var_99': current_var_99,\n        'expected_shortfall_95': current_es_95,\n        'portfolio_volatility': portfolio_returns.std() * np.sqrt(252)\n    },\n    'regime_analysis': {\n        'current_regime': current_regime_label,\n        'regime_stability': 'High' if regime_changes <= 2 else 'Medium' if regime_changes <= 5 else 'Low',\n        'recent_changes': int(regime_changes)\n    },\n    'stress_test_summary': {\n        'worst_historical_loss': worst_historical_loss,\n        'average_mc_var_99': avg_mc_var_99\n    },\n    'alert_status': alert_status,\n    'recommendations': recommendations\n}\n\nwith open('../data/risk_management_summary.pkl', 'wb') as f:\n    pickle.dump(risk_management_summary, f)\n\nprint(\"\\nRisk management summary saved to '../data/risk_management_summary.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
