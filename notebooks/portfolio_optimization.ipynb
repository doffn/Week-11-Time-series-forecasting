{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Optimization Using Modern Portfolio Theory\n",
    "\n",
    "This notebook covers:\n",
    "- Expected returns calculation using forecasting results\n",
    "- Risk modeling and covariance estimation\n",
    "- Efficient frontier generation\n",
    "- Portfolio optimization with constraints\n",
    "- Risk-adjusted performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Portfolio optimization libraries\n",
    "import scipy.optimize as sco\n",
    "from scipy import stats\n",
    "import cvxpy as cp\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models, expected_returns\n",
    "from pypfopt.discrete_allocation import DiscreteAllocation\n",
    "from pypfopt.cla import CLA\n",
    "\n",
    "# Custom modules\n",
    "from src.data.loader import DataLoader\n",
    "from src.portfolio.optimizer import PortfolioOptimizer\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logger(__name__)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Portfolio optimization libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Forecasting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data\n",
    "loader = DataLoader()\n",
    "data = loader.load_data(['TSLA', 'BND', 'SPY'], '2015-07-01', '2025-07-31')\n",
    "prices = data['prices']\n",
    "returns = data['returns']\n",
    "\n",
    "# Load forecasting results\n",
    "forecasting_results = {}\n",
    "\n",
    "# Load ARIMA results\n",
    "try:\n",
    "    with open('../data/arima_results.pkl', 'rb') as f:\n",
    "        forecasting_results['arima'] = pickle.load(f)\n",
    "    print(\"✓ ARIMA results loaded\")\nexcept FileNotFoundError:\n",
    "    print(\"✗ ARIMA results not found\")\n",
    "    forecasting_results['arima'] = None\n",
    "\n",
    "# Load LSTM results\n",
    "try:\n",
    "    with open('../data/lstm_results.pkl', 'rb') as f:\n",
    "        forecasting_results['lstm'] = pickle.load(f)\n",
    "    print(\"✓ LSTM results loaded\")\nexcept FileNotFoundError:\n",
    "    print(\"✗ LSTM results not found\")\n",
    "    forecasting_results['lstm'] = None\n",
    "\n",
    "# Display available forecasting results\n",
    "print(f\"\\nAvailable forecasting models:\")\n",
    "for model, results in forecasting_results.items():\n",
    "    if results is not None:\n",
    "        accuracy = results['forecast_accuracy']['rmse']\n",
    "        target = results['long_term_forecast']['price_target']\n",
    "        expected_return = results['long_term_forecast']['expected_return']\n",
    "        print(f\"  {model.upper()}: RMSE={accuracy:.4f}, Target=${target:.2f}, Return={expected_return:.2f}%\")\n",
    "\n",
    "# Asset information\n",
    "assets = ['TSLA', 'BND', 'SPY']\n",
    "asset_names = {\n",
    "    'TSLA': 'Tesla Inc.',\n",
    "    'BND': 'Vanguard Total Bond Market ETF',\n",
    "    'SPY': 'SPDR S&P 500 ETF'\n",
    "}\n",
    "\n",
    "print(f\"\\nPortfolio Assets:\")\n",
    "for asset in assets:\n",
    "    current_price = prices[asset].iloc[-1]\n",
    "    print(f\"  {asset} ({asset_names[asset]}): ${current_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Returns Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedReturnsCalculator:\n",
    "    \"\"\"\n",
    "    Calculate expected returns using various methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data, prices_data):\n",
    "        self.returns = returns_data\n",
    "        self.prices = prices_data\n",
    "        \n",
    "    def historical_mean(self, lookback_days=252):\n",
    "        \"\"\"\n",
    "        Calculate expected returns using historical mean\n",
    "        \"\"\"\n",
    "        if lookback_days:\n",
    "            recent_returns = self.returns.tail(lookback_days)\n",
    "        else:\n",
    "            recent_returns = self.returns\n",
    "            \n",
    "        return recent_returns.mean() * 252  # Annualized\n",
    "    \n",
    "    def capm_returns(self, risk_free_rate=0.02, market_asset='SPY'):\n",
    "        \"\"\"\n",
    "        Calculate expected returns using CAPM\n",
    "        \"\"\"\n",
    "        market_return = self.historical_mean()[market_asset]\n",
    "        market_premium = market_return - risk_free_rate\n",
    "        \n",
    "        expected_returns = {}\n",
    "        \n",
    "        for asset in self.returns.columns:\n",
    "            if asset == market_asset:\n",
    "                expected_returns[asset] = market_return\n",
    "            else:\n",
    "                # Calculate beta\n",
    "                covariance = self.returns[asset].cov(self.returns[market_asset]) * 252\n",
    "                market_variance = self.returns[market_asset].var() * 252\n",
    "                beta = covariance / market_variance\n",
    "                \n",
    "                # CAPM expected return\n",
    "                expected_returns[asset] = risk_free_rate + beta * market_premium\n",
    "                \n",
    "        return pd.Series(expected_returns)\n",
    "    \n",
    "    def forecast_based_returns(self, forecasting_results, confidence_level=0.8):\n",
    "        \"\"\"\n",
    "        Calculate expected returns using forecasting results\n",
    "        \"\"\"\n",
    "        expected_returns = {}\n",
    "        \n",
    "        # TSLA: Use best available forecast\n",
    "        tsla_forecasts = []\n",
    "        \n",
    "        for model_name, results in forecasting_results.items():\n",
    "            if results is not None:\n",
    "                forecast_return = results['long_term_forecast']['expected_return'] / 100\n",
    "                accuracy_weight = 1 / (results['forecast_accuracy']['rmse'] + 1e-6)\n",
    "                tsla_forecasts.append((forecast_return, accuracy_weight))\n",
    "        \n",
    "        if tsla_forecasts:\n",
    "            # Weighted average of forecasts\n",
    "            total_weight = sum(weight for _, weight in tsla_forecasts)\n",
    "            weighted_return = sum(ret * weight for ret, weight in tsla_forecasts) / total_weight\n",
    "            expected_returns['TSLA'] = weighted_return\n",
    "        else:\n",
    "            # Fallback to historical mean\n",
    "            expected_returns['TSLA'] = self.historical_mean()['TSLA']\n",
    "        \n",
    "        # Other assets: Use historical mean or CAPM\n",
    "        capm_returns = self.capm_returns()\n",
    "        for asset in ['BND', 'SPY']:\n",
    "            expected_returns[asset] = capm_returns[asset]\n",
    "            \n",
    "        return pd.Series(expected_returns)\n",
    "    \n",
    "    def shrinkage_estimator(self, shrinkage_factor=0.2):\n",
    "        \"\"\"\n",
    "        James-Stein shrinkage estimator for expected returns\n",
    "        \"\"\"\n",
    "        historical_returns = self.historical_mean()\n",
    "        grand_mean = historical_returns.mean()\n",
    "        \n",
    "        # Shrink towards grand mean\n",
    "        shrunk_returns = (1 - shrinkage_factor) * historical_returns + shrinkage_factor * grand_mean\n",
    "        \n",
    "        return shrunk_returns\n",
    "\n",
    "# Initialize calculator\n",
    "returns_calc = ExpectedReturnsCalculator(returns, prices)\n",
    "\n",
    "# Calculate expected returns using different methods\n",
    "expected_returns_methods = {\n",
    "    'Historical Mean': returns_calc.historical_mean(),\n",
    "    'CAPM': returns_calc.capm_returns(),\n",
    "    'Forecast-Based': returns_calc.forecast_based_returns(forecasting_results),\n",
    "    'Shrinkage': returns_calc.shrinkage_estimator()\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "comparison_df = pd.DataFrame(expected_returns_methods)\n",
    "comparison_df = comparison_df * 100  # Convert to percentage\n",
    "\n",
    "print(\"EXPECTED RETURNS COMPARISON (Annualized %)\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# Select primary method for optimization\n",
    "primary_method = 'Forecast-Based' if any(forecasting_results.values()) else 'CAPM'\n",
    "expected_returns_final = expected_returns_methods[primary_method]\n",
    "\n",
    "print(f\"\\nSelected method for optimization: {primary_method}\")\n",
    "print(f\"Final expected returns:\")\n",
    "for asset, ret in expected_returns_final.items():\n",
    "    print(f\"  {asset}: {ret*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Modeling and Covariance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskModelEstimator:\n",
    "    \"\"\"\n",
    "    Estimate risk models and covariance matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns_data):\n",
    "        self.returns = returns_data\n",
    "        \n",
    "    def sample_covariance(self, frequency=252):\n",
    "        \"\"\"\n",
    "        Simple sample covariance matrix\n",
    "        \"\"\"\n",
    "        return self.returns.cov() * frequency\n",
    "    \n",
    "    def exponential_covariance(self, span=60, frequency=252):\n",
    "        \"\"\"\n",
    "        Exponentially weighted covariance matrix\n",
    "        \"\"\"\n",
    "        return self.returns.ewm(span=span).cov().iloc[-len(self.returns.columns):] * frequency\n",
    "    \n",
    "    def shrinkage_covariance(self, shrinkage_factor=0.2, frequency=252):\n",
    "        \"\"\"\n",
    "        Ledoit-Wolf shrinkage covariance estimator\n",
    "        \"\"\"\n",
    "        sample_cov = self.sample_covariance(frequency)\n",
    "        \n",
    "        # Shrinkage target: diagonal matrix with average variance\n",
    "        avg_variance = np.trace(sample_cov) / len(sample_cov)\n",
    "        shrinkage_target = np.eye(len(sample_cov)) * avg_variance\n",
    "        \n",
    "        # Shrunk covariance\n",
    "        shrunk_cov = (1 - shrinkage_factor) * sample_cov + shrinkage_factor * shrinkage_target\n",
    "        \n",
    "        return shrunk_cov\n",
    "    \n",
    "    def rolling_covariance(self, window=252, frequency=252):\n",
    "        \"\"\"\n",
    "        Rolling window covariance (uses most recent window)\n",
    "        \"\"\"\n",
    "        recent_returns = self.returns.tail(window)\n",
    "        return recent_returns.cov() * frequency\n",
    "    \n",
    "    def factor_model_covariance(self, market_factor='SPY', frequency=252):\n",
    "        \"\"\"\n",
    "        Single factor model covariance\n",
    "        \"\"\"\n",
    "        # Calculate factor loadings (betas)\n",
    "        market_returns = self.returns[market_factor]\n",
    "        market_var = market_returns.var() * frequency\n",
    "        \n",
    "        betas = {}\n",
    "        residual_vars = {}\n",
    "        \n",
    "        for asset in self.returns.columns:\n",
    "            if asset == market_factor:\n",
    "                betas[asset] = 1.0\n",
    "                residual_vars[asset] = 0.0\n",
    "            else:\n",
    "                # Calculate beta\n",
    "                covariance = self.returns[asset].cov(market_returns) * frequency\n",
    "                betas[asset] = covariance / market_var\n",
    "                \n",
    "                # Calculate residual variance\n",
    "                asset_var = self.returns[asset].var() * frequency\n",
    "                residual_vars[asset] = asset_var - (betas[asset] ** 2) * market_var\n",
    "        \n",
    "        # Construct covariance matrix\n",
    "        n_assets = len(self.returns.columns)\n",
    "        cov_matrix = np.zeros((n_assets, n_assets))\n",
    "        \n",
    "        for i, asset_i in enumerate(self.returns.columns):\n",
    "            for j, asset_j in enumerate(self.returns.columns):\n",
    "                if i == j:\n",
    "                    # Diagonal: factor variance + residual variance\n",
    "                    cov_matrix[i, j] = (betas[asset_i] ** 2) * market_var + residual_vars[asset_i]\n",
    "                else:\n",
    "                    # Off-diagonal: factor covariance only\n",
    "                    cov_matrix[i, j] = betas[asset_i] * betas[asset_j] * market_var\n",
    "        \n",
    "        return pd.DataFrame(cov_matrix, index=self.returns.columns, columns=self.returns.columns)\n",
    "\n",
    "# Initialize risk model estimator\n",
    "risk_estimator = RiskModelEstimator(returns)\n",
    "\n",
    "# Calculate covariance matrices using different methods\n",
    "covariance_methods = {\n",
    "    'Sample': risk_estimator.sample_covariance(),\n",
    "    'Exponential': risk_estimator.exponential_covariance(),\n",
    "    'Shrinkage': risk_estimator.shrinkage_covariance(),\n",
    "    'Rolling': risk_estimator.rolling_covariance(),\n",
    "    'Factor Model': risk_estimator.factor_model_covariance()\n",
    "}\n",
    "\n",
    "# Display covariance matrices\n",
    "print(\"COVARIANCE MATRIX COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for method, cov_matrix in covariance_methods.items():\n",
    "    print(f\"\\n{method} Covariance Matrix:\")\n",
    "    print(cov_matrix.round(6))\n",
    "    \n",
    "    # Calculate condition number (numerical stability)\n",
    "    condition_number = np.linalg.cond(cov_matrix)\n",
    "    print(f\"Condition Number: {condition_number:.2f}\")\n",
    "\n",
    "# Select primary covariance matrix\n",
    "primary_cov_method = 'Shrinkage'  # Generally more stable\n",
    "covariance_matrix = covariance_methods[primary_cov_method]\n",
    "\n",
    "print(f\"\\nSelected covariance method: {primary_cov_method}\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "correlation_matrix = returns.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            square=True, fmt='.3f', cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Asset Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk decomposition\n",
    "print(f\"\\nRISK ANALYSIS:\")\n",
    "print(f\"-\" * 30)\n",
    "for asset in assets:\n",
    "    volatility = np.sqrt(covariance_matrix.loc[asset, asset])\n",
    "    print(f\"{asset} Volatility: {volatility*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nCORRELATION ANALYSIS:\")\n",
    "print(f\"-\" * 30)\n",
    "for i, asset1 in enumerate(assets):\n",
    "    for asset2 in assets[i+1:]:\n",
    "        corr = correlation_matrix.loc[asset1, asset2]\n",
    "        print(f\"{asset1}-{asset2} Correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Frontier Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientFrontierGenerator:\n",
    "    \"\"\"\n",
    "    Generate efficient frontier using various optimization approaches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expected_returns, covariance_matrix, risk_free_rate=0.02):\n",
    "        self.mu = expected_returns\n",
    "        self.S = covariance_matrix\n",
    "        self.rf = risk_free_rate\n",
    "        self.assets = list(expected_returns.index)\n",
    "        self.n_assets = len(self.assets)\n",
    "        \n",
    "    def portfolio_stats(self, weights):\n",
    "        \"\"\"\n",
    "        Calculate portfolio statistics\n",
    "        \"\"\"\n",
    "        weights = np.array(weights)\n",
    "        portfolio_return = np.sum(weights * self.mu)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(self.S, weights)))\n",
    "        sharpe_ratio = (portfolio_return - self.rf) / portfolio_volatility\n",
    "        \n",
    "        return portfolio_return, portfolio_volatility, sharpe_ratio\n",
    "    \n",
    "    def minimize_volatility(self, target_return=None, constraints=None):\n",
    "        \"\"\"\n",
    "        Find minimum volatility portfolio\n",
    "        \"\"\"\n",
    "        def objective(weights):\n",
    "            return np.sqrt(np.dot(weights.T, np.dot(self.S, weights)))\n",
    "        \n",
    "        # Constraints\n",
    "        cons = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]  # Weights sum to 1\n",
    "        \n",
    "        if target_return is not None:\n",
    "            cons.append({'type': 'eq', 'fun': lambda x: np.sum(x * self.mu) - target_return})\n",
    "        \n",
    "        if constraints:\n",
    "            cons.extend(constraints)\n",
    "        \n",
    "        # Bounds (no short selling)\n",
    "        bounds = tuple((0, 1) for _ in range(self.n_assets))\n",
    "        \n",
    "        # Initial guess\n",
    "        x0 = np.array([1/self.n_assets] * self.n_assets)\n",
    "        \n",
    "        # Optimize\n",
    "        result = sco.minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def maximize_sharpe(self, constraints=None):\n",
    "        \"\"\"\n",
    "        Find maximum Sharpe ratio portfolio\n",
    "        \"\"\"\n",
    "        def objective(weights):\n",
    "            portfolio_return, portfolio_volatility, sharpe_ratio = self.portfolio_stats(weights)\n",
    "            return -sharpe_ratio  # Minimize negative Sharpe ratio\n",
    "        \n",
    "        # Constraints\n",
    "        cons = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
    "        \n",
    "        if constraints:\n",
    "            cons.extend(constraints)\n",
    "        \n",
    "        # Bounds\n",
    "        bounds = tuple((0, 1) for _ in range(self.n_assets))\n",
    "        \n",
    "        # Initial guess\n",
    "        x0 = np.array([1/self.n_assets] * self.n_assets)\n",
    "        \n",
    "        # Optimize\n",
    "        result = sco.minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_frontier(self, n_portfolios=100, constraints=None):\n",
    "        \"\"\"\n",
    "        Generate efficient frontier\n",
    "        \"\"\"\n",
    "        # Find minimum and maximum return portfolios\n",
    "        min_vol_result = self.minimize_volatility(constraints=constraints)\n",
    "        min_ret = np.sum(min_vol_result.x * self.mu)\n",
    "        max_ret = np.max(self.mu)\n",
    "        \n",
    "        # Generate target returns\n",
    "        target_returns = np.linspace(min_ret, max_ret * 0.95, n_portfolios)\n",
    "        \n",
    "        # Optimize for each target return\n",
    "        frontier_portfolios = []\n",
    "        \n",
    "        for target_ret in target_returns:\n",
    "            try:\n",
    "                result = self.minimize_volatility(target_return=target_ret, constraints=constraints)\n",
    "                if result.success:\n",
    "                    ret, vol, sharpe = self.portfolio_stats(result.x)\n",
    "                    frontier_portfolios.append({\n",
    "                        'return': ret,\n",
    "                        'volatility': vol,\n",
    "                        'sharpe': sharpe,\n",
    "                        'weights': result.x\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(frontier_portfolios)\n",
    "    \n",
    "    def monte_carlo_portfolios(self, n_portfolios=10000):\n",
    "        \"\"\"\n",
    "        Generate random portfolios for comparison\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _ in range(n_portfolios):\n",
    "            # Generate random weights\n",
    "            weights = np.random.random(self.n_assets)\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "            # Calculate portfolio stats\n",
    "            ret, vol, sharpe = self.portfolio_stats(weights)\n",
    "            \n",
    "            results.append({\n",
    "                'return': ret,\n",
    "                'volatility': vol,\n",
    "                'sharpe': sharpe,\n",
    "                'weights': weights\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize efficient frontier generator\n",
    "ef_generator = EfficientFrontierGenerator(expected_returns_final, covariance_matrix)\n",
    "\n",
    "# Generate efficient frontier\n",
    "print(\"Generating efficient frontier...\")\n",
    "efficient_frontier = ef_generator.generate_frontier(n_portfolios=50)\n",
    "\n",
    "# Find optimal portfolios\n",
    "min_vol_result = ef_generator.minimize_volatility()\n",
    "max_sharpe_result = ef_generator.maximize_sharpe()\n",
    "\n",
    "# Calculate optimal portfolio statistics\n",
    "min_vol_stats = ef_generator.portfolio_stats(min_vol_result.x)\n",
    "max_sharpe_stats = ef_generator.portfolio_stats(max_sharpe_result.x)\n",
    "\n",
    "print(f\"\\nOPTIMAL PORTFOLIOS\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "print(f\"\\nMinimum Volatility Portfolio:\")\n",
    "print(f\"Expected Return: {min_vol_stats[0]*100:.2f}%\")\n",
    "print(f\"Volatility: {min_vol_stats[1]*100:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {min_vol_stats[2]:.3f}\")\n",
    "print(f\"Weights:\")\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"  {asset}: {min_vol_result.x[i]*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nMaximum Sharpe Ratio Portfolio:\")\n",
    "print(f\"Expected Return: {max_sharpe_stats[0]*100:.2f}%\")\n",
    "print(f\"Volatility: {max_sharpe_stats[1]*100:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {max_sharpe_stats[2]:.3f}\")\n",
    "print(f\"Weights:\")\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"  {asset}: {max_sharpe_result.x[i]*100:.2f}%\")\n",
    "\n",
    "# Generate Monte Carlo portfolios for visualization\n",
    "print(\"\\nGenerating Monte Carlo portfolios for visualization...\")\n",
    "mc_portfolios = ef_generator.monte_carlo_portfolios(n_portfolios=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize efficient frontier\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "fig.suptitle('Portfolio Optimization Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Efficient frontier plot\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Plot Monte Carlo portfolios\n",
    "scatter = ax1.scatter(mc_portfolios['volatility']*100, mc_portfolios['return']*100, \n",
    "                     c=mc_portfolios['sharpe'], cmap='viridis', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter, ax=ax1, label='Sharpe Ratio')\n",
    "\n",
    "# Plot efficient frontier\n",
    "ax1.plot(efficient_frontier['volatility']*100, efficient_frontier['return']*100, \n",
    "         'r-', linewidth=3, label='Efficient Frontier')\n",
    "\n",
    "# Plot optimal portfolios\n",
    "ax1.scatter(min_vol_stats[1]*100, min_vol_stats[0]*100, \n",
    "           marker='*', color='blue', s=500, label='Min Volatility', edgecolors='black')\n",
    "ax1.scatter(max_sharpe_stats[1]*100, max_sharpe_stats[0]*100, \n",
    "           marker='*', color='red', s=500, label='Max Sharpe', edgecolors='black')\n",
    "\n",
    "# Plot individual assets\n",
    "for asset in assets:\n",
    "    asset_return = expected_returns_final[asset] * 100\n",
    "    asset_vol = np.sqrt(covariance_matrix.loc[asset, asset]) * 100\n",
    "    ax1.scatter(asset_vol, asset_return, marker='o', s=200, label=asset, edgecolors='black')\n",
    "\n",
    "ax1.set_xlabel('Volatility (%)')\n",
    "ax1.set_ylabel('Expected Return (%)')\n",
    "ax1.set_title('Efficient Frontier')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Portfolio composition pie charts\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Create subplot for pie charts\n",
    "ax2.axis('off')\n",
    "\n",
    "# Max Sharpe portfolio composition\n",
    "pie_ax1 = fig.add_subplot(2, 4, 6)\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "pie_ax1.pie(max_sharpe_result.x, labels=assets, autopct='%1.1f%%', \n",
    "           colors=colors, startangle=90)\n",
    "pie_ax1.set_title('Max Sharpe Portfolio', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Min Vol portfolio composition\n",
    "pie_ax2 = fig.add_subplot(2, 4, 8)\n",
    "pie_ax2.pie(min_vol_result.x, labels=assets, autopct='%1.1f%%', \n",
    "           colors=colors, startangle=90)\n",
    "pie_ax2.set_title('Min Volatility Portfolio', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk-return statistics table\n",
    "portfolio_comparison = pd.DataFrame({\n",
    "    'Min Volatility': [min_vol_stats[0]*100, min_vol_stats[1]*100, min_vol_stats[2]],\n",
    "    'Max Sharpe': [max_sharpe_stats[0]*100, max_sharpe_stats[1]*100, max_sharpe_stats[2]]\n",
    "}, index=['Expected Return (%)', 'Volatility (%)', 'Sharpe Ratio'])\n",
    "\n",
    "print(\"\\nPORTFOLIO COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "print(portfolio_comparison.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Portfolio Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define various constraint scenarios\n",
    "constraint_scenarios = {\n",
    "    'Unconstrained': [],\n",
    "    'Max 60% Single Asset': [{'type': 'ineq', 'fun': lambda x: 0.6 - x[i]} for i in range(len(assets))],\n",
    "    'Min 10% Each Asset': [{'type': 'ineq', 'fun': lambda x: x[i] - 0.1} for i in range(len(assets))],\n",
    "    'Max 40% TSLA': [{'type': 'ineq', 'fun': lambda x: 0.4 - x[0]}],  # TSLA is first asset\n",
    "    'Min 20% Bonds': [{'type': 'ineq', 'fun': lambda x: x[1] - 0.2}]   # BND is second asset\n",
    "}\n",
    "\n",
    "# Optimize portfolios under different constraints\n",
    "constrained_results = {}\n",
    "\n",
    "print(\"CONSTRAINED PORTFOLIO OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario_name, constraints in constraint_scenarios.items():\n",
    "    print(f\"\\n{scenario_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Optimize for maximum Sharpe ratio\n",
    "        result = ef_generator.maximize_sharpe(constraints=constraints)\n",
    "        \n",
    "        if result.success:\n",
    "            stats = ef_generator.portfolio_stats(result.x)\n",
    "            constrained_results[scenario_name] = {\n",
    "                'weights': result.x,\n",
    "                'return': stats[0],\n",
    "                'volatility': stats[1],\n",
    "                'sharpe': stats[2]\n",
    "            }\n",
    "            \n",
    "            print(f\"  Expected Return: {stats[0]*100:.2f}%\")\n",
    "            print(f\"  Volatility: {stats[1]*100:.2f}%\")\n",
    "            print(f\"  Sharpe Ratio: {stats[2]:.3f}\")\n",
    "            print(f\"  Weights: {', '.join([f'{asset}={w*100:.1f}%' for asset, w in zip(assets, result.x)])}\")\n",
    "        else:\n",
    "            print(f\"  Optimization failed: {result.message}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Compare constrained portfolios\n",
    "if constrained_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for scenario, results in constrained_results.items():\n",
    "        row = [scenario, results['return']*100, results['volatility']*100, results['sharpe']]\n",
    "        for i, asset in enumerate(assets):\n",
    "            row.append(results['weights'][i]*100)\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    columns = ['Scenario', 'Return (%)', 'Volatility (%)', 'Sharpe'] + [f'{asset} (%)' for asset in assets]\n",
    "    comparison_df = pd.DataFrame(comparison_data, columns=columns)\n",
    "    \n",
    "    print(\"\\nCONSTRAINED PORTFOLIO COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Budgeting and Alternative Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlternativePortfolioStrategies:\n",
    "    \"\"\"\n",
    "    Alternative portfolio construction approaches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expected_returns, covariance_matrix):\n",
    "        self.mu = expected_returns\n",
    "        self.S = covariance_matrix\n",
    "        self.assets = list(expected_returns.index)\n",
    "        \n",
    "    def equal_weight(self):\n",
    "        \"\"\"\n",
    "        Equal weight portfolio (1/N)\n",
    "        \"\"\"\n",
    "        n = len(self.assets)\n",
    "        return np.array([1/n] * n)\n",
    "    \n",
    "    def inverse_volatility(self):\n",
    "        \"\"\"\n",
    "        Inverse volatility weighted portfolio\n",
    "        \"\"\"\n",
    "        volatilities = np.sqrt(np.diag(self.S))\n",
    "        inv_vol_weights = 1 / volatilities\n",
    "        return inv_vol_weights / np.sum(inv_vol_weights)\n",
    "    \n",
    "    def risk_parity(self, max_iter=1000, tol=1e-8):\n",
    "        \"\"\"\n",
    "        Risk parity portfolio (equal risk contribution)\n",
    "        \"\"\"\n",
    "        n = len(self.assets)\n",
    "        \n",
    "        def risk_budget_objective(weights):\n",
    "            portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(self.S, weights)))\n",
    "            marginal_contrib = np.dot(self.S, weights) / portfolio_vol\n",
    "            contrib = weights * marginal_contrib\n",
    "            \n",
    "            # Target: equal risk contribution (1/n each)\n",
    "            target_contrib = portfolio_vol / n\n",
    "            return np.sum((contrib - target_contrib) ** 2)\n",
    "        \n",
    "        # Constraints and bounds\n",
    "        constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
    "        bounds = tuple((0.001, 0.999) for _ in range(n))  # Avoid zero weights\n",
    "        \n",
    "        # Initial guess\n",
    "        x0 = np.array([1/n] * n)\n",
    "        \n",
    "        # Optimize\n",
    "        result = sco.minimize(risk_budget_objective, x0, method='SLSQP', \n",
    "                            bounds=bounds, constraints=constraints,\n",
    "                            options={'maxiter': max_iter, 'ftol': tol})\n",
    "        \n",
    "        return result.x if result.success else self.equal_weight()\n",
    "    \n",
    "    def maximum_diversification(self):\n",
    "        \"\"\"\n",
    "        Maximum diversification portfolio\n",
    "        \"\"\"\n",
    "        def diversification_ratio(weights):\n",
    "            # Weighted average volatility\n",
    "            individual_vols = np.sqrt(np.diag(self.S))\n",
    "            weighted_avg_vol = np.sum(weights * individual_vols)\n",
    "            \n",
    "            # Portfolio volatility\n",
    "            portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(self.S, weights)))\n",
    "            \n",
    "            # Maximize diversification ratio (minimize negative)\n",
    "            return -weighted_avg_vol / portfolio_vol\n",
    "        \n",
    "        n = len(self.assets)\n",
    "        constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
    "        bounds = tuple((0, 1) for _ in range(n))\n",
    "        x0 = np.array([1/n] * n)\n",
    "        \n",
    "        result = sco.minimize(diversification_ratio, x0, method='SLSQP',\n",
    "                            bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        return result.x if result.success else self.equal_weight()\n",
    "    \n",
    "    def black_litterman(self, tau=0.025, P=None, Q=None, omega=None):\n",
    "        \"\"\"\n",
    "        Black-Litterman model (simplified implementation)\n",
    "        \"\"\"\n",
    "        # Market capitalization weights (proxy using equal weights)\n",
    "        w_market = self.equal_weight()\n",
    "        \n",
    "        # Implied equilibrium returns\n",
    "        # Assuming risk aversion coefficient of 3\n",
    "        risk_aversion = 3\n",
    "        pi = risk_aversion * np.dot(self.S, w_market)\n",
    "        \n",
    "        if P is None or Q is None:\n",
    "            # No views, return equilibrium portfolio\n",
    "            return w_market\n",
    "        \n",
    "        # Black-Litterman formula\n",
    "        tau_S = tau * self.S\n",
    "        \n",
    "        # New expected returns\n",
    "        M1 = np.linalg.inv(tau_S)\n",
    "        M2 = np.dot(P.T, np.dot(np.linalg.inv(omega), P))\n",
    "        M3 = np.dot(np.linalg.inv(tau_S), pi)\n",
    "        M4 = np.dot(P.T, np.dot(np.linalg.inv(omega), Q))\n",
    "        \n",
    "        mu_bl = np.dot(np.linalg.inv(M1 + M2), M3 + M4)\n",
    "        \n",
    "        # New covariance matrix\n",
    "        S_bl = np.linalg.inv(M1 + M2)\n",
    "        \n",
    "        # Optimize with Black-Litterman inputs\n",
    "        ef_bl = EfficientFrontierGenerator(pd.Series(mu_bl, index=self.assets), \n",
    "                                         pd.DataFrame(S_bl, index=self.assets, columns=self.assets))\n",
    "        result = ef_bl.maximize_sharpe()\n",
    "        \n",
    "        return result.x if result.success else w_market\n",
    "\n",
    "# Initialize alternative strategies\n",
    "alt_strategies = AlternativePortfolioStrategies(expected_returns_final, covariance_matrix)\n",
    "\n",
    "# Calculate alternative portfolio weights\n",
    "alternative_portfolios = {\n",
    "    'Equal Weight': alt_strategies.equal_weight(),\n",
    "    'Inverse Volatility': alt_strategies.inverse_volatility(),\n",
    "    'Risk Parity': alt_strategies.risk_parity(),\n",
    "    'Max Diversification': alt_strategies.maximum_diversification()\n",
    "}\n",
    "\n",
    "# Calculate statistics for alternative portfolios\n",
    "print(\"ALTERNATIVE PORTFOLIO STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "alternative_stats = {}\n",
    "\n",
    "for strategy_name, weights in alternative_portfolios.items():\n",
    "    stats = ef_generator.portfolio_stats(weights)\n",
    "    alternative_stats[strategy_name] = {\n",
    "        'weights': weights,\n",
    "        'return': stats[0],\n",
    "        'volatility': stats[1],\n",
    "        'sharpe': stats[2]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Expected Return: {stats[0]*100:.2f}%\")\n",
    "    print(f\"  Volatility: {stats[1]*100:.2f}%\")\n",
    "    print(f\"  Sharpe Ratio: {stats[2]:.3f}\")\n",
    "    print(f\"  Weights: {', '.join([f'{asset}={w*100:.1f}%' for asset, w in zip(assets, weights)])}\")\n",
    "\n",
    "# Add traditional optimal portfolios for comparison\n",
    "alternative_stats['Max Sharpe (Traditional)'] = {\n",
    "    'weights': max_sharpe_result.x,\n",
    "    'return': max_sharpe_stats[0],\n",
    "    'volatility': max_sharpe_stats[1],\n",
    "    'sharpe': max_sharpe_stats[2]\n",
    "}\n",
    "\n",
    "alternative_stats['Min Volatility (Traditional)'] = {\n",
    "    'weights': min_vol_result.x,\n",
    "    'return': min_vol_stats[0],\n",
    "    'volatility': min_vol_stats[1],\n",
    "    'sharpe': min_vol_stats[2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all strategies\n",
    "all_strategies_data = []\n",
    "\n",
    "for strategy, stats in alternative_stats.items():\n",
    "    row = [strategy, stats['return']*100, stats['volatility']*100, stats['sharpe']]\n",
    "    for i, asset in enumerate(assets):\n",
    "        row.append(stats['weights'][i]*100)\n",
    "    all_strategies_data.append(row)\n",
    "\n",
    "columns = ['Strategy', 'Return (%)', 'Volatility (%)', 'Sharpe'] + [f'{asset} (%)' for asset in assets]\n",
    "all_strategies_df = pd.DataFrame(all_strategies_data, columns=columns)\n",
    "\n",
    "# Sort by Sharpe ratio\n",
    "all_strategies_df = all_strategies_df.sort_values('Sharpe', ascending=False)\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE STRATEGY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(all_strategies_df.round(2).to_string(index=False))\n",
    "\n",
    "# Visualize strategy comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Portfolio Strategy Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Risk-Return scatter\n",
    "ax1 = axes[0, 0]\n",
    "for strategy, stats in alternative_stats.items():\n",
    "    ax1.scatter(stats['volatility']*100, stats['return']*100, s=100, label=strategy)\n",
    "\n",
    "ax1.set_xlabel('Volatility (%)')\n",
    "ax1.set_ylabel('Expected Return (%)')\n",
    "ax1.set_title('Risk-Return Profile')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpe ratio comparison\n",
    "ax2 = axes[0, 1]\n",
    "strategies = list(alternative_stats.keys())\n",
    "sharpe_ratios = [stats['sharpe'] for stats in alternative_stats.values()]\n",
    "\n",
    "bars = ax2.bar(range(len(strategies)), sharpe_ratios, color='skyblue', alpha=0.7)\n",
    "ax2.set_xlabel('Strategy')\n",
    "ax2.set_ylabel('Sharpe Ratio')\n",
    "ax2.set_title('Sharpe Ratio Comparison')\n",
    "ax2.set_xticks(range(len(strategies)))\n",
    "ax2.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, sharpe_ratios):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Portfolio weights heatmap\n",
    "ax3 = axes[1, 0]\n",
    "weights_matrix = np.array([stats['weights'] for stats in alternative_stats.values()])\n",
    "im = ax3.imshow(weights_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "ax3.set_xticks(range(len(assets)))\n",
    "ax3.set_xticklabels(assets)\n",
    "ax3.set_yticks(range(len(strategies)))\n",
    "ax3.set_yticklabels(strategies)\n",
    "ax3.set_title('Portfolio Weights Heatmap')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3)\n",
    "cbar.set_label('Weight')\n",
    "\n",
    "# Add weight values as text\n",
    "for i in range(len(strategies)):\n",
    "    for j in range(len(assets)):\n",
    "        text = ax3.text(j, i, f'{weights_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "# Risk contribution analysis for selected strategies\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Calculate risk contributions for Max Sharpe portfolio\n",
    "max_sharpe_weights = alternative_stats['Max Sharpe (Traditional)']['weights']\n",
    "portfolio_vol = np.sqrt(np.dot(max_sharpe_weights.T, np.dot(covariance_matrix, max_sharpe_weights)))\n",
    "marginal_contrib = np.dot(covariance_matrix, max_sharpe_weights) / portfolio_vol\n",
    "risk_contrib = max_sharpe_weights * marginal_contrib\n",
    "risk_contrib_pct = risk_contrib / np.sum(risk_contrib) * 100\n",
    "\n",
    "ax4.pie(risk_contrib_pct, labels=assets, autopct='%1.1f%%', startangle=90)\n",
    "ax4.set_title('Risk Contribution\\n(Max Sharpe Portfolio)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk contribution analysis\n",
    "print(f\"\\nRISK CONTRIBUTION ANALYSIS (Max Sharpe Portfolio)\")\n",
    "print(f\"=\" * 50)\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"{asset}: {risk_contrib_pct[i]:.2f}% risk contribution, {max_sharpe_weights[i]*100:.2f}% weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Portfolio Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select recommended portfolio based on analysis\n",
    "best_strategy = all_strategies_df.iloc[0]['Strategy']\n",
    "recommended_portfolio = alternative_stats[best_strategy]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PORTFOLIO RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nRECOMMENDED STRATEGY: {best_strategy}\")\n",
    "print(f\"\\nRATIONALE:\")\n",
    "print(f\"• Highest Sharpe ratio: {recommended_portfolio['sharpe']:.3f}\")\n",
    "print(f\"• Optimal risk-adjusted returns\")\n",
    "print(f\"• Incorporates forecasting insights\")\n",
    "print(f\"• Balances diversification and performance\")\n",
    "\n",
    "print(f\"\\nPORTFOLIO SPECIFICATIONS:\")\n",
    "print(f\"-\" * 40)\n",
    "print(f\"Expected Annual Return: {recommended_portfolio['return']*100:.2f}%\")\n",
    "print(f\"Expected Annual Volatility: {recommended_portfolio['volatility']*100:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {recommended_portfolio['sharpe']:.3f}\")\n",
    "print(f\"Maximum Drawdown (estimated): {recommended_portfolio['volatility']*100*1.5:.2f}%\")\n",
    "\n",
    "print(f\"\\nASSET ALLOCATION:\")\n",
    "print(f\"-\" * 40)\n",
    "total_allocation = 0\n",
    "for i, asset in enumerate(assets):\n",
    "    weight = recommended_portfolio['weights'][i]\n",
    "    print(f\"{asset} ({asset_names[asset]}): {weight*100:.2f}%\")\n",
    "    total_allocation += weight\n",
    "\n",
    "print(f\"\\nTotal Allocation: {total_allocation*100:.2f}%\")\n",
    "\n",
    "# Calculate portfolio metrics\n",
    "current_prices = {asset: prices[asset].iloc[-1] for asset in assets}\n",
    "portfolio_value = 100000  # $100,000 example portfolio\n",
    "\n",
    "print(f\"\\nPORTFOLIO IMPLEMENTATION (${portfolio_value:,} investment):\")\n",
    "print(f\"-\" * 50)\n",
    "for i, asset in enumerate(assets):\n",
    "    weight = recommended_portfolio['weights'][i]\n",
    "    dollar_amount = portfolio_value * weight\n",
    "    shares = dollar_amount / current_prices[asset]\n",
    "    print(f\"{asset}: ${dollar_amount:,.0f} ({shares:.0f} shares at ${current_prices[asset]:.2f})\")\n",
    "\n",
    "# Risk metrics\n",
    "portfolio_var_95 = stats.norm.ppf(0.05) * recommended_portfolio['volatility'] * np.sqrt(1/252) * portfolio_value\n",
    "portfolio_var_99 = stats.norm.ppf(0.01) * recommended_portfolio['volatility'] * np.sqrt(1/252) * portfolio_value\n",
    "\n",
    "print(f\"\\nRISK METRICS:\")\n",
    "print(f\"-\" * 40)\n",
    "print(f\"Daily VaR (95%): ${abs(portfolio_var_95):,.0f}\")\n",
    "print(f\"Daily VaR (99%): ${abs(portfolio_var_99):,.0f}\")\n",
    "print(f\"Expected daily volatility: ${recommended_portfolio['volatility'] * np.sqrt(1/252) * portfolio_value:,.0f}\")\n",
    "\n",
    "# Performance projections\n",
    "print(f\"\\nPERFORMANCE PROJECTIONS:\")\n",
    "print(f\"-\" * 40)\n",
    "annual_return = recommended_portfolio['return']\n",
    "annual_vol = recommended_portfolio['volatility']\n",
    "\n",
    "# Monte Carlo simulation for performance distribution\n",
    "np.random.seed(42)\n",
    "n_simulations = 10000\n",
    "annual_returns = np.random.normal(annual_return, annual_vol, n_simulations)\n",
    "final_values = portfolio_value * (1 + annual_returns)\n",
    "\n",
    "print(f\"1-Year Performance Distribution (10,000 simulations):\")\n",
    "print(f\"  Expected Value: ${np.mean(final_values):,.0f}\")\n",
    "print(f\"  5th Percentile: ${np.percentile(final_values, 5):,.0f}\")\n",
    "print(f\"  25th Percentile: ${np.percentile(final_values, 25):,.0f}\")\n",
    "print(f\"  75th Percentile: ${np.percentile(final_values, 75):,.0f}\")\n",
    "print(f\"  95th Percentile: ${np.percentile(final_values, 95):,.0f}\")\n",
    "print(f\"  Probability of Loss: {np.mean(final_values < portfolio_value)*100:.1f}%\")\n",
    "\n",
    "# Implementation recommendations\n",
    "print(f\"\\nIMPLEMENTATION RECOMMENDATIONS:\")\n",
    "print(f\"-\" * 40)\n",
    "print(f\"• Rebalance portfolio monthly or when weights deviate >5%\")\n",
    "print(f\"• Monitor forecasting model performance and update quarterly\")\n",
    "print(f\"• Consider transaction costs in rebalancing decisions\")\n",
    "print(f\"• Review and update expected returns semi-annually\")\n",
    "print(f\"• Implement stop-loss rules for individual positions if desired\")\n",
    "print(f\"• Consider tax implications for taxable accounts\")\n",
    "\n",
    "print(f\"\\nRISK MANAGEMENT:\")\n",
    "print(f\"-\" * 40)\n",
    "print(f\"• Set maximum portfolio loss limit (e.g., 15-20%)\")\n",
    "print(f\"• Monitor correlation changes between assets\")\n",
    "print(f\"• Be prepared to adjust allocation during market stress\")\n",
    "print(f\"• Consider adding defensive assets during high volatility periods\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PORTFOLIO OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save portfolio recommendation\n",
    "portfolio_recommendation = {\n",
    "    'strategy': best_strategy,\n",
    "    'weights': dict(zip(assets, recommended_portfolio['weights'])),\n",
    "    'expected_return': recommended_portfolio['return'],\n",
    "    'expected_volatility': recommended_portfolio['volatility'],\n",
    "    'sharpe_ratio': recommended_portfolio['sharpe'],\n",
    "    'risk_metrics': {\n",
    "        'var_95': abs(portfolio_var_95),\n",
    "        'var_99': abs(portfolio_var_99)\n",
    "    },\n",
    "    'performance_projections': {\n",
    "        'expected_value': np.mean(final_values),\n",
    "        'percentile_5': np.percentile(final_values, 5),\n",
    "        'percentile_95': np.percentile(final_values, 95),\n",
    "        'probability_of_loss': np.mean(final_values < portfolio_value)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/portfolio_recommendation.pkl', 'wb') as f:\n",
    "    pickle.dump(portfolio_recommendation, f)\n",
    "\n",
    "print(\"\\nPortfolio recommendation saved to '../data/portfolio_recommendation.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter":  "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.9.0"\n",
  }\n",
 },\n",
 "nbformat": 4,\n",
 "nbformat_minor": 4\n",
}
