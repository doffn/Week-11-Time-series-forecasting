{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting for Portfolio Management Optimization\n",
    "## GMF Investments - Financial Analysis Challenge\n",
    "\n",
    "**Analyst:** [Your Name]  \n",
    "**Date:** August 2025  \n",
    "**Period:** July 1, 2015 - July 31, 2025  \n",
    "\n",
    "### Executive Summary\n",
    "This analysis implements time series forecasting models to optimize portfolio allocation across three key assets: Tesla (TSLA), Vanguard Total Bond Market ETF (BND), and S&P 500 ETF (SPY). We develop both statistical (ARIMA) and deep learning (LSTM) models to forecast future market trends and apply Modern Portfolio Theory for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.optimize as sco\n",
    "from scipy import stats\n",
    "\n",
    "# Time series and forecasting\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Deep learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Portfolio optimization\n",
    "import pypfopt\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing and Exploration\n",
    "\n",
    "### 1.1 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the assets and time period\n",
    "assets = ['TSLA', 'BND', 'SPY']\n",
    "start_date = '2015-07-01'\n",
    "end_date = '2025-07-31'\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading financial data...\")\n",
    "data = {}\n",
    "for asset in assets:\n",
    "    try:\n",
    "        ticker = yf.Ticker(asset)\n",
    "        data[asset] = ticker.history(start=start_date, end=end_date)\n",
    "        print(f\"✓ {asset}: {len(data[asset])} records downloaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {asset}: {e}\")\n",
    "\n",
    "# Create a combined dataframe with adjusted close prices\n",
    "price_data = pd.DataFrame()\n",
    "for asset in assets:\n",
    "    if asset in data:\n",
    "        price_data[asset] = data[asset]['Close']\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {price_data.shape}\")\n",
    "print(f\"Date range: {price_data.index.min()} to {price_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleaning and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per asset:\")\n",
    "print(price_data.isnull().sum())\n",
    "\n",
    "# Handle missing values by forward filling\n",
    "price_data = price_data.fillna(method='ffill').dropna()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(price_data.describe())\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData Types:\")\n",
    "print(price_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Returns and Risk Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "returns = price_data.pct_change().dropna()\n",
    "\n",
    "# Calculate rolling volatility (30-day window)\n",
    "rolling_vol = returns.rolling(window=30).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Calculate cumulative returns\n",
    "cumulative_returns = (1 + returns).cumprod()\n",
    "\n",
    "print(\"Returns Statistics:\")\n",
    "print(returns.describe())\n",
    "\n",
    "# Calculate key risk metrics\n",
    "def calculate_risk_metrics(returns_series, confidence_level=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk (VaR) and Sharpe Ratio\n",
    "    \"\"\"\n",
    "    # VaR calculation\n",
    "    var = np.percentile(returns_series, confidence_level * 100)\n",
    "    \n",
    "    # Sharpe Ratio (assuming risk-free rate of 2%)\n",
    "    risk_free_rate = 0.02 / 252  # Daily risk-free rate\n",
    "    excess_returns = returns_series - risk_free_rate\n",
    "    sharpe_ratio = excess_returns.mean() / returns_series.std() * np.sqrt(252)\n",
    "    \n",
    "    return var, sharpe_ratio\n",
    "\n",
    "print(\"\\nRisk Metrics:\")\n",
    "for asset in assets:\n",
    "    var, sharpe = calculate_risk_metrics(returns[asset])\n",
    "    print(f\"{asset}: VaR (5%) = {var:.4f}, Sharpe Ratio = {sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('Comprehensive Financial Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Price Evolution\n",
    "ax1 = axes[0, 0]\n",
    "for asset in assets:\n",
    "    ax1.plot(price_data.index, price_data[asset], label=asset, linewidth=1.5)\n",
    "ax1.set_title('Price Evolution Over Time')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Normalized Price Evolution\n",
    "ax2 = axes[0, 1]\n",
    "normalized_prices = price_data / price_data.iloc[0]\n",
    "for asset in assets:\n",
    "    ax2.plot(normalized_prices.index, normalized_prices[asset], label=asset, linewidth=1.5)\n",
    "ax2.set_title('Normalized Price Evolution (Base = 1)')\n",
    "ax2.set_ylabel('Normalized Price')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Daily Returns Distribution\n",
    "ax3 = axes[1, 0]\n",
    "returns['TSLA'].hist(bins=50, alpha=0.7, ax=ax3, color='red')\n",
    "ax3.set_title('TSLA Daily Returns Distribution')\n",
    "ax3.set_xlabel('Daily Returns')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.axvline(returns['TSLA'].mean(), color='black', linestyle='--', label=f'Mean: {returns[\"TSLA\"].mean():.4f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Rolling Volatility\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(rolling_vol.index, rolling_vol['TSLA'], label='TSLA', color='red', linewidth=1.5)\n",
    "ax4.set_title('TSLA Rolling Volatility (30-day)')\n",
    "ax4.set_ylabel('Annualized Volatility')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Correlation Heatmap\n",
    "ax5 = axes[2, 0]\n",
    "correlation_matrix = returns.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax5)\n",
    "ax5.set_title('Asset Correlation Matrix')\n",
    "\n",
    "# 6. Cumulative Returns\n",
    "ax6 = axes[2, 1]\n",
    "for asset in assets:\n",
    "    ax6.plot(cumulative_returns.index, cumulative_returns[asset], label=asset, linewidth=1.5)\n",
    "ax6.set_title('Cumulative Returns')\n",
    "ax6.set_ylabel('Cumulative Return')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, title=''):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'\\n{title}')\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"✓ Series is stationary (reject null hypothesis)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"✗ Series is non-stationary (fail to reject null hypothesis)\")\n",
    "        return False\n",
    "\n",
    "print(\"STATIONARITY TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test stationarity for prices\n",
    "for asset in assets:\n",
    "    adf_test(price_data[asset], f'{asset} Price Series')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING RETURNS (FIRST DIFFERENCE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test stationarity for returns\n",
    "for asset in assets:\n",
    "    adf_test(returns[asset], f'{asset} Returns Series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers(series, multiplier=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "print(\"OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for asset in assets:\n",
    "    outliers = detect_outliers(returns[asset])\n",
    "    print(f\"\\n{asset}:\")\n",
    "    print(f\"Number of outliers: {len(outliers)}\")\n",
    "    print(f\"Percentage of outliers: {len(outliers)/len(returns[asset])*100:.2f}%\")\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"Extreme outliers:\")\n",
    "        extreme_outliers = outliers.nlargest(3).append(outliers.nsmallest(3))\n",
    "        for date, value in extreme_outliers.items():\n",
    "            print(f\"  {date.strftime('%Y-%m-%d')}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Time Series Forecasting Models\n",
    "\n",
    "### 2.1 Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on TSLA for forecasting\n",
    "tsla_prices = price_data['TSLA'].copy()\n",
    "tsla_returns = returns['TSLA'].copy()\n",
    "\n",
    "# Split data chronologically\n",
    "split_date = '2024-01-01'\n",
    "train_prices = tsla_prices[tsla_prices.index < split_date]\n",
    "test_prices = tsla_prices[tsla_prices.index >= split_date]\n",
    "\n",
    "train_returns = tsla_returns[tsla_returns.index < split_date]\n",
    "test_returns = tsla_returns[tsla_returns.index >= split_date]\n",
    "\n",
    "print(f\"Training set: {len(train_prices)} observations\")\n",
    "print(f\"Test set: {len(test_prices)} observations\")\n",
    "print(f\"Training period: {train_prices.index.min()} to {train_prices.index.max()}\")\n",
    "print(f\"Test period: {test_prices.index.min()} to {test_prices.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ARIMA Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA for optimal parameters\n",
    "print(\"Finding optimal ARIMA parameters...\")\n",
    "auto_model = auto_arima(train_returns, \n",
    "                       start_p=0, start_q=0, \n",
    "                       max_p=5, max_q=5, \n",
    "                       seasonal=False, \n",
    "                       stepwise=True, \n",
    "                       suppress_warnings=True,\n",
    "                       error_action='ignore')\n",
    "\n",
    "print(f\"Optimal ARIMA order: {auto_model.order}\")\n",
    "print(auto_model.summary())\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(train_returns, order=auto_model.order)\n",
    "arima_fitted = arima_model.fit()\n",
    "\n",
    "# Generate forecasts\n",
    "n_periods = len(test_returns)\n",
    "arima_forecast = arima_fitted.forecast(steps=n_periods)\n",
    "arima_conf_int = arima_fitted.get_forecast(steps=n_periods).conf_int()\n",
    "\n",
    "print(f\"\\nARIMA forecast generated for {n_periods} periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LSTM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(data, look_back=60):\n",
    "    \"\"\"\n",
    "    Create dataset for LSTM with look_back window\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_prices = scaler.fit_transform(train_prices.values.reshape(-1, 1))\n",
    "\n",
    "# Create LSTM dataset\n",
    "look_back = 60\n",
    "X_train, y_train = create_lstm_dataset(scaled_prices.flatten(), look_back)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(f\"LSTM training data shape: {X_train.shape}\")\n",
    "print(f\"LSTM target data shape: {y_train.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(look_back, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "print(\"LSTM model architecture:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "history = lstm_model.fit(X_train, y_train, \n",
    "                        batch_size=32, \n",
    "                        epochs=50, \n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LSTM predictions\n",
    "# Prepare test data\n",
    "all_prices = pd.concat([train_prices, test_prices])\n",
    "scaled_all_prices = scaler.fit_transform(all_prices.values.reshape(-1, 1))\n",
    "\n",
    "# Create test dataset\n",
    "test_start_idx = len(train_prices) - look_back\n",
    "test_data = scaled_all_prices[test_start_idx:]\n",
    "\n",
    "X_test = []\n",
    "for i in range(look_back, len(test_data)):\n",
    "    X_test.append(test_data[i-look_back:i])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions\n",
    "lstm_predictions_scaled = lstm_model.predict(X_test)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "\n",
    "print(f\"LSTM predictions generated: {len(lstm_predictions)} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ARIMA return forecasts to price forecasts\n",
    "arima_price_forecast = []\n",
    "last_price = train_prices.iloc[-1]\n",
    "\n",
    "for return_forecast in arima_forecast:\n",
    "    next_price = last_price * (1 + return_forecast)\n",
    "    arima_price_forecast.append(next_price)\n",
    "    last_price = next_price\n",
    "\n",
    "arima_price_forecast = np.array(arima_price_forecast)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "# Ensure same length for comparison\n",
    "min_length = min(len(test_prices), len(arima_price_forecast), len(lstm_predictions))\n",
    "actual_prices = test_prices.iloc[:min_length].values\n",
    "arima_pred = arima_price_forecast[:min_length]\n",
    "lstm_pred = lstm_predictions[:min_length].flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "arima_mae, arima_rmse, arima_mape = calculate_metrics(actual_prices, arima_pred)\n",
    "lstm_mae, lstm_rmse, lstm_mape = calculate_metrics(actual_prices, lstm_pred)\n",
    "\n",
    "# Results comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'LSTM'],\n",
    "    'MAE': [arima_mae, lstm_mae],\n",
    "    'RMSE': [arima_rmse, lstm_rmse],\n",
    "    'MAPE (%)': [arima_mape, lstm_mape]\n",
    "})\n",
    "\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_model = 'ARIMA' if arima_rmse < lstm_rmse else 'LSTM'\n",
    "print(f\"\\nBest performing model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(train_prices.index[-200:], train_prices.iloc[-200:], \n",
    "         label='Historical Prices', color='blue', linewidth=2)\n",
    "\n",
    "# Plot actual test prices\n",
    "plt.plot(test_prices.index[:min_length], actual_prices, \n",
    "         label='Actual Prices', color='black', linewidth=2)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(test_prices.index[:min_length], arima_pred, \n",
    "         label=f'ARIMA Forecast (RMSE: {arima_rmse:.2f})', \n",
    "         color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.plot(test_prices.index[:min_length], lstm_pred, \n",
    "         label=f'LSTM Forecast (RMSE: {lstm_rmse:.2f})', \n",
    "         color='green', linewidth=2, linestyle='-.')\n",
    "\n",
    "plt.title('TSLA Price Forecasting: Model Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Future Market Trends Forecasting\n",
    "\n",
    "### 3.1 Generate 6-12 Month Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate future forecasts (6-12 months)\n",
    "forecast_periods = 252  # Approximately 12 months of trading days\n",
    "future_dates = pd.date_range(start=tsla_prices.index[-1] + timedelta(days=1), \n",
    "                           periods=forecast_periods, freq='B')\n",
    "\n",
    "# Use the best performing model for future forecasts\n",
    "if best_model == 'ARIMA':\n",
    "    # Refit ARIMA on full dataset\n",
    "    full_arima = ARIMA(tsla_returns, order=auto_model.order)\n",
    "    full_arima_fitted = full_arima.fit()\n",
    "    \n",
    "    # Generate future return forecasts\n",
    "    future_returns_forecast = full_arima_fitted.forecast(steps=forecast_periods)\n",
    "    future_conf_int = full_arima_fitted.get_forecast(steps=forecast_periods).conf_int()\n",
    "    \n",
    "    # Convert to price forecasts\n",
    "    future_prices_forecast = []\n",
    "    future_prices_lower = []\n",
    "    future_prices_upper = []\n",
    "    \n",
    "    last_price = tsla_prices.iloc[-1]\n",
    "    \n",
    "    for i, return_forecast in enumerate(future_returns_forecast):\n",
    "        # Point forecast\n",
    "        next_price = last_price * (1 + return_forecast)\n",
    "        future_prices_forecast.append(next_price)\n",
    "        \n",
    "        # Confidence intervals\n",
    "        lower_return = future_conf_int.iloc[i, 0]\n",
    "        upper_return = future_conf_int.iloc[i, 1]\n",
    "        \n",
    "        future_prices_lower.append(last_price * (1 + lower_return))\n",
    "        future_prices_upper.append(last_price * (1 + upper_return))\n",
    "        \n",
    "        last_price = next_price\n",
    "    \n",
    "    future_prices_forecast = np.array(future_prices_forecast)\n",
    "    future_prices_lower = np.array(future_prices_lower)\n",
    "    future_prices_upper = np.array(future_prices_upper)\n",
    "    \n",
    "else:\n",
    "    # Use LSTM for future forecasts\n",
    "    # This is a simplified approach - in practice, you'd want to retrain on full data\n",
    "    future_prices_forecast = lstm_pred  # Placeholder\n",
    "    future_prices_lower = future_prices_forecast * 0.9  # Simple confidence interval\n",
    "    future_prices_upper = future_prices_forecast * 1.1\n",
    "\n",
    "print(f\"Generated {forecast_periods} future price forecasts using {best_model} model\")\n",
    "print(f\"Forecast period: {future_dates[0]} to {future_dates[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Forecast Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive forecast visualization\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Historical data (last 2 years)\n",
    "historical_cutoff = tsla_prices.index[-500:]\n",
    "plt.plot(historical_cutoff, tsla_prices.loc[historical_cutoff], \n",
    "         label='Historical Prices', color='blue', linewidth=2)\n",
    "\n",
    "# Future forecasts\n",
    "plt.plot(future_dates, future_prices_forecast, \n",
    "         label=f'{best_model} Forecast', color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(future_dates, future_prices_lower, future_prices_upper, \n",
    "                alpha=0.3, color='red', label='95% Confidence Interval')\n",
    "\n",
    "plt.title('TSLA 12-Month Price Forecast', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Forecast statistics\n",
    "current_price = tsla_prices.iloc[-1]\n",
    "forecast_end_price = future_prices_forecast[-1]\n",
    "total_return = (forecast_end_price - current_price) / current_price * 100\n",
    "\n",
    "print(\"\\nFORECAST ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Current TSLA Price: ${current_price:.2f}\")\n",
    "print(f\"12-Month Forecast: ${forecast_end_price:.2f}\")\n",
    "print(f\"Expected Total Return: {total_return:.2f}%\")\n",
    "print(f\"Annualized Return: {total_return:.2f}%\")\n",
    "\n",
    "# Confidence interval analysis\n",
    "ci_width_start = future_prices_upper[0] - future_prices_lower[0]\n",
    "ci_width_end = future_prices_upper[-1] - future_prices_lower[-1]\n",
    "ci_expansion = (ci_width_end - ci_width_start) / ci_width_start * 100\n",
    "\n",
    "print(f\"\\nCONFIDENCE INTERVAL ANALYSIS\")\n",
    "print(f\"Initial CI Width: ${ci_width_start:.2f}\")\n",
    "print(f\"Final CI Width: ${ci_width_end:.2f}\")\n",
    "print(f\"CI Expansion: {ci_expansion:.1f}%\")\n",
    "print(\"\\nImplication: Uncertainty increases significantly over longer forecast horizons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Portfolio Optimization\n",
    "\n",
    "### 4.1 Expected Returns Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected returns\n",
    "# TSLA: Use forecast-based expected return\n",
    "tsla_expected_return = total_return / 100  # Convert percentage to decimal\n",
    "\n",
    "# BND and SPY: Use historical average returns\n",
    "historical_returns_annual = returns.mean() * 252\n",
    "bnd_expected_return = historical_returns_annual['BND']\n",
    "spy_expected_return = historical_returns_annual['SPY']\n",
    "\n",
    "# Create expected returns vector\n",
    "expected_returns_vector = pd.Series({\n",
    "    'TSLA': tsla_expected_return,\n",
    "    'BND': bnd_expected_return,\n",
    "    'SPY': spy_expected_return\n",
    "})\n",
    "\n",
    "print(\"EXPECTED RETURNS (ANNUALIZED)\")\n",
    "print(\"=\" * 40)\n",
    "for asset, ret in expected_returns_vector.items():\n",
    "    print(f\"{asset}: {ret:.4f} ({ret*100:.2f}%)\")\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = returns.cov() * 252  # Annualized\n",
    "\n",
    "print(\"\\nCOVARIANCE MATRIX (ANNUALIZED)\")\n",
    "print(\"=\" * 40)\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Efficient Frontier Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Efficient Frontier\n",
    "def generate_efficient_frontier(expected_returns, cov_matrix, num_portfolios=10000):\n",
    "    \"\"\"\n",
    "    Generate efficient frontier using Monte Carlo simulation\n",
    "    \"\"\"\n",
    "    num_assets = len(expected_returns)\n",
    "    results = np.zeros((3, num_portfolios))\n",
    "    weights_array = np.zeros((num_portfolios, num_assets))\n",
    "    \n",
    "    # Risk-free rate (2% annual)\n",
    "    risk_free_rate = 0.02\n",
    "    \n",
    "    for i in range(num_portfolios):\n",
    "        # Generate random weights\n",
    "        weights = np.random.random(num_assets)\n",
    "        weights /= np.sum(weights)  # Normalize to sum to 1\n",
    "        weights_array[i] = weights\n",
    "        \n",
    "        # Calculate portfolio return and volatility\n",
    "        portfolio_return = np.sum(weights * expected_returns)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        \n",
    "        # Calculate Sharpe ratio\n",
    "        sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_volatility\n",
    "        \n",
    "        # Store results\n",
    "        results[0, i] = portfolio_return\n",
    "        results[1, i] = portfolio_volatility\n",
    "        results[2, i] = sharpe_ratio\n",
    "    \n",
    "    return results, weights_array\n",
    "\n",
    "# Generate efficient frontier\n",
    "print(\"Generating Efficient Frontier...\")\n",
    "results, weights = generate_efficient_frontier(expected_returns_vector, cov_matrix)\n",
    "\n",
    "# Find optimal portfolios\n",
    "max_sharpe_idx = np.argmax(results[2])\n",
    "min_vol_idx = np.argmin(results[1])\n",
    "\n",
    "# Maximum Sharpe Ratio Portfolio\n",
    "max_sharpe_return = results[0, max_sharpe_idx]\n",
    "max_sharpe_volatility = results[1, max_sharpe_idx]\n",
    "max_sharpe_ratio = results[2, max_sharpe_idx]\n",
    "max_sharpe_weights = weights[max_sharpe_idx]\n",
    "\n",
    "# Minimum Volatility Portfolio\n",
    "min_vol_return = results[0, min_vol_idx]\n",
    "min_vol_volatility = results[1, min_vol_idx]\n",
    "min_vol_ratio = results[2, min_vol_idx]\n",
    "min_vol_weights = weights[min_vol_idx]\n",
    "\n",
    "print(\"Efficient Frontier generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Efficient Frontier Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient frontier plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all portfolios\n",
    "plt.scatter(results[1], results[0], c=results[2], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "\n",
    "# Highlight optimal portfolios\n",
    "plt.scatter(max_sharpe_volatility, max_sharpe_return, \n",
    "           marker='*', color='red', s=500, label='Maximum Sharpe Ratio')\n",
    "plt.scatter(min_vol_volatility, min_vol_return, \n",
    "           marker='*', color='blue', s=500, label='Minimum Volatility')\n",
    "\n",
    "# Plot individual assets\n",
    "for i, asset in enumerate(assets):\n",
    "    asset_return = expected_returns_vector[asset]\n",
    "    asset_vol = np.sqrt(cov_matrix.loc[asset, asset])\n",
    "    plt.scatter(asset_vol, asset_return, marker='o', s=200, label=asset)\n",
    "\n",
    "plt.title('Efficient Frontier', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Volatility (Risk)')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display optimal portfolio details\n",
    "print(\"\\nOPTIMAL PORTFOLIOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nMaximum Sharpe Ratio Portfolio:\")\n",
    "print(f\"Expected Return: {max_sharpe_return:.4f} ({max_sharpe_return*100:.2f}%)\")\n",
    "print(f\"Volatility: {max_sharpe_volatility:.4f} ({max_sharpe_volatility*100:.2f}%)\")\n",
    "print(f\"Sharpe Ratio: {max_sharpe_ratio:.4f}\")\n",
    "print(\"Weights:\")\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"  {asset}: {max_sharpe_weights[i]:.4f} ({max_sharpe_weights[i]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nMinimum Volatility Portfolio:\")\n",
    "print(f\"Expected Return: {min_vol_return:.4f} ({min_vol_return*100:.2f}%)\")\n",
    "print(f\"Volatility: {min_vol_volatility:.4f} ({min_vol_volatility*100:.2f}%)\")\n",
    "print(f\"Sharpe Ratio: {min_vol_ratio:.4f}\")\n",
    "print(\"Weights:\")\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"  {asset}: {min_vol_weights[i]:.4f} ({min_vol_weights[i]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Portfolio Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select recommended portfolio (Maximum Sharpe Ratio)\n",
    "recommended_weights = max_sharpe_weights\n",
    "recommended_return = max_sharpe_return\n",
    "recommended_volatility = max_sharpe_volatility\n",
    "recommended_sharpe = max_sharpe_ratio\n",
    "\n",
    "print(\"PORTFOLIO RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nRecommended Portfolio: Maximum Sharpe Ratio Portfolio\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"- Maximizes risk-adjusted returns\")\n",
    "print(\"- Optimal balance between return and risk\")\n",
    "print(\"- Suitable for investors seeking efficient risk-return trade-off\")\n",
    "\n",
    "print(\"\\nPortfolio Composition:\")\n",
    "portfolio_df = pd.DataFrame({\n",
    "    'Asset': assets,\n",
    "    'Weight': recommended_weights,\n",
    "    'Weight (%)': recommended_weights * 100,\n",
    "    'Expected Return': expected_returns_vector.values,\n",
    "    'Expected Return (%)': expected_returns_vector.values * 100\n",
    "})\n",
    "\n",
    "print(portfolio_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "print(f\"\\nPortfolio Metrics:\")\n",
    "print(f\"Expected Annual Return: {recommended_return:.4f} ({recommended_return*100:.2f}%)\")\n",
    "print(f\"Annual Volatility: {recommended_volatility:.4f} ({recommended_volatility*100:.2f}%)\")\n",
    "print(f\"Sharpe Ratio: {recommended_sharpe:.4f}\")\n",
    "\n",
    "# Create portfolio composition pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "plt.pie(recommended_weights, labels=assets, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Recommended Portfolio Composition', fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Strategy Backtesting\n",
    "\n",
    "### 5.1 Backtesting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backtesting period (last year of data)\n",
    "backtest_start = '2024-08-01'\n",
    "backtest_end = '2025-07-31'\n",
    "\n",
    "# Get backtesting data\n",
    "backtest_prices = price_data[price_data.index >= backtest_start].copy()\n",
    "backtest_returns = returns[returns.index >= backtest_start].copy()\n",
    "\n",
    "print(f\"Backtesting period: {backtest_start} to {backtest_end}\")\n",
    "print(f\"Backtesting data points: {len(backtest_prices)}\")\n",
    "\n",
    "# Define portfolios\n",
    "# Strategy portfolio: Our optimized portfolio\n",
    "strategy_weights = pd.Series(recommended_weights, index=assets)\n",
    "\n",
    "# Benchmark portfolio: 60% SPY, 40% BND\n",
    "benchmark_weights = pd.Series([0.0, 0.4, 0.6], index=assets)  # [TSLA, BND, SPY]\n",
    "\n",
    "print(\"\\nPortfolio Weights:\")\n",
    "print(\"Strategy Portfolio:\")\n",
    "for asset, weight in strategy_weights.items():\n",
    "    print(f\"  {asset}: {weight:.4f} ({weight*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nBenchmark Portfolio (60/40 SPY/BND):\")\n",
    "for asset, weight in benchmark_weights.items():\n",
    "    print(f\"  {asset}: {weight:.4f} ({weight*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Portfolio Performance Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio returns\n",
    "def calculate_portfolio_returns(returns_df, weights):\n",
    "    \"\"\"\n",
    "    Calculate portfolio returns given asset returns and weights\n",
    "    \"\"\"\n",
    "    return (returns_df * weights).sum(axis=1)\n",
    "\n",
    "# Calculate daily portfolio returns\n",
    "strategy_returns = calculate_portfolio_returns(backtest_returns, strategy_weights)\n",
    "benchmark_returns = calculate_portfolio_returns(backtest_returns, benchmark_weights)\n",
    "\n",
    "# Calculate cumulative returns\n",
    "strategy_cumulative = (1 + strategy_returns).cumprod()\n",
    "benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
    "\n",
    "# Calculate performance metrics\n",
    "def calculate_performance_metrics(returns_series, risk_free_rate=0.02):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \"\"\"\n",
    "    # Annualized return\n",
    "    total_return = (1 + returns_series).prod() - 1\n",
    "    annualized_return = (1 + total_return) ** (252 / len(returns_series)) - 1\n",
    "    \n",
    "    # Annualized volatility\n",
    "    annualized_volatility = returns_series.std() * np.sqrt(252)\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    excess_returns = returns_series - risk_free_rate / 252\n",
    "    sharpe_ratio = excess_returns.mean() / returns_series.std() * np.sqrt(252)\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + returns_series).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    return {\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Annualized Volatility': annualized_volatility,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Maximum Drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Calculate metrics for both portfolios\n",
    "strategy_metrics = calculate_performance_metrics(strategy_returns)\n",
    "benchmark_metrics = calculate_performance_metrics(benchmark_returns)\n",
    "\n",
    "print(\"BACKTESTING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Strategy Portfolio': strategy_metrics,\n",
    "    'Benchmark Portfolio': benchmark_metrics\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Portfolio Backtesting Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Cumulative Returns\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(strategy_cumulative.index, strategy_cumulative, \n",
    "         label='Strategy Portfolio', linewidth=2, color='blue')\n",
    "ax1.plot(benchmark_cumulative.index, benchmark_cumulative, \n",
    "         label='Benchmark Portfolio', linewidth=2, color='red')\n",
    "ax1.set_title('Cumulative Returns')\n",
    "ax1.set_ylabel('Cumulative Return')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Rolling Sharpe Ratio (30-day)\n",
    "ax2 = axes[0, 1]\n",
    "strategy_rolling_sharpe = strategy_returns.rolling(30).mean() / strategy_returns.rolling(30).std() * np.sqrt(252)\n",
    "benchmark_rolling_sharpe = benchmark_returns.rolling(30).mean() / benchmark_returns.rolling(30).std() * np.sqrt(252)\n",
    "\n",
    "ax2.plot(strategy_rolling_sharpe.index, strategy_rolling_sharpe, \n",
    "         label='Strategy Portfolio', linewidth=2, color='blue')\n",
    "ax2.plot(benchmark_rolling_sharpe.index, benchmark_rolling_sharpe, \n",
    "         label='Benchmark Portfolio', linewidth=2, color='red')\n",
    "ax2.set_title('Rolling Sharpe Ratio (30-day)')\n",
    "ax2.set_ylabel('Sharpe Ratio')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Drawdown Analysis\n",
    "ax3 = axes[1, 0]\n",
    "strategy_cumulative_max = strategy_cumulative.expanding().max()\n",
    "strategy_drawdown = (strategy_cumulative - strategy_cumulative_max) / strategy_cumulative_max\n",
    "\n",
    "benchmark_cumulative_max = benchmark_cumulative.expanding().max()\n",
    "benchmark_drawdown = (benchmark_cumulative - benchmark_cumulative_max) / benchmark_cumulative_max\n",
    "\n",
    "ax3.fill_between(strategy_drawdown.index, strategy_drawdown, 0, \n",
    "                alpha=0.7, color='blue', label='Strategy Portfolio')\n",
    "ax3.fill_between(benchmark_drawdown.index, benchmark_drawdown, 0, \n",
    "                alpha=0.7, color='red', label='Benchmark Portfolio')\n",
    "ax3.set_title('Drawdown Analysis')\n",
    "ax3.set_ylabel('Drawdown')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Monthly Returns Distribution\n",
    "ax4 = axes[1, 1]\n",
    "strategy_monthly = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "benchmark_monthly = benchmark_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "\n",
    "ax4.hist(strategy_monthly, bins=10, alpha=0.7, label='Strategy Portfolio', color='blue')\n",
    "ax4.hist(benchmark_monthly, bins=10, alpha=0.7, label='Benchmark Portfolio', color='red')\n",
    "ax4.set_title('Monthly Returns Distribution')\n",
    "ax4.set_xlabel('Monthly Return')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Backtesting Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison and conclusions\n",
    "strategy_outperformed = strategy_metrics['Total Return'] > benchmark_metrics['Total Return']\n",
    "strategy_better_sharpe = strategy_metrics['Sharpe Ratio'] > benchmark_metrics['Sharpe Ratio']\n",
    "\n",
    "print(\"BACKTESTING CONCLUSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nStrategy vs Benchmark Performance:\")\n",
    "print(f\"Total Return: Strategy {strategy_metrics['Total Return']:.4f} vs Benchmark {benchmark_metrics['Total Return']:.4f}\")\n",
    "print(f\"Sharpe Ratio: Strategy {strategy_metrics['Sharpe Ratio']:.4f} vs Benchmark {benchmark_metrics['Sharpe Ratio']:.4f}\")\n",
    "print(f\"Max Drawdown: Strategy {strategy_metrics['Maximum Drawdown']:.4f} vs Benchmark {benchmark_metrics['Maximum Drawdown']:.4f}\")\n",
    "\n",
    "print(f\"\\nOutperformance Analysis:\")\n",
    "if strategy_outperformed:\n",
    "    outperformance = (strategy_metrics['Total Return'] - benchmark_metrics['Total Return']) * 100\n",
    "    print(f\"✓ Strategy outperformed benchmark by {outperformance:.2f} percentage points\")\nelse:\n",
    "    underperformance = (benchmark_metrics['Total Return'] - strategy_metrics['Total Return']) * 100\n",
    "    print(f\"✗ Strategy underperformed benchmark by {underperformance:.2f} percentage points\")\n",
    "\n",
    "if strategy_better_sharpe:\n",
    "    print(f\"✓ Strategy achieved better risk-adjusted returns (higher Sharpe ratio)\")\nelse:\n",
    "    print(f\"✗ Benchmark achieved better risk-adjusted returns (higher Sharpe ratio)\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"1. Model-driven approach {'validated' if strategy_outperformed else 'challenged'} by backtesting results\")\n",
    "print(f\"2. TSLA forecast {'contributed positively' if strategy_outperformed else 'may have introduced excess risk'} to portfolio performance\")\n",
    "print(f\"3. Diversification across asset classes {'effectively managed' if strategy_metrics['Maximum Drawdown'] > benchmark_metrics['Maximum Drawdown'] else 'helped manage'} downside risk\")\n",
    "print(f\"4. {'Higher' if strategy_metrics['Annualized Volatility'] > benchmark_metrics['Annualized Volatility'] else 'Lower'} volatility suggests {'more aggressive' if strategy_metrics['Annualized Volatility'] > benchmark_metrics['Annualized Volatility'] else 'more conservative'} risk profile\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "if strategy_outperformed:\n",
    "    print(\"• Continue with model-driven approach but implement regular rebalancing\")\n",
    "    print(\"• Monitor forecast accuracy and adjust weights accordingly\")\n",
    "    print(\"• Consider expanding to more assets for better diversification\")\nelse:\n",
    "    print(\"• Review forecasting model parameters and methodology\")\n",
    "    print(\"• Consider reducing allocation to high-volatility assets like TSLA\")\n",
    "    print(\"• Implement more frequent rebalancing to capture changing market conditions\")\n",
    "\n",
    "print(\"\\nLimitations of this backtest:\")\n",
    "print(\"• Single year testing period may not capture all market conditions\")\n",
    "print(\"• Transaction costs and slippage not considered\")\n",
    "print(\"• Model was not retrained during the backtesting period\")\n",
    "print(\"• Survivorship bias - only tested on assets that continued trading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary and Investment Memo\n",
    "\n",
    "### Final Recommendations for GMF Investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final investment memo summary\n",
    "print(\"=\"*80)\n",
    "print(\"GMF INVESTMENTS - INVESTMENT MEMO\")\n",
    "print(\"Time Series Forecasting for Portfolio Optimization\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%B %d, %Y')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nEXECUTIVE SUMMARY\")\n",
    "print(\"-\"*50)\n",
    "print(\"Our analysis employed advanced time series forecasting models (ARIMA and LSTM)\")\n",
    "print(\"to predict Tesla (TSLA) price movements and optimize portfolio allocation across\")\n",
    "print(\"three key assets: TSLA, BND (bonds), and SPY (broad market).\")\n",
    "\n",
    "print(f\"\\nKEY FINDINGS\")\n",
    "print(\"-\"*50)\n",
    "print(f\"• Best performing model: {best_model} (RMSE: {min(arima_rmse, lstm_rmse):.2f})\")\n",
    "print(f\"• 12-month TSLA forecast: ${forecast_end_price:.2f} ({total_return:+.1f}% expected return)\")\n",
    "print(f\"• Optimal portfolio Sharpe ratio: {recommended_sharpe:.3f}\")\n",
    "print(f\"• Backtesting {'validated' if strategy_outperformed else 'challenged'} our approach\")\n",
    "\n",
    "print(f\"\\nRECOMMENDED PORTFOLIO ALLOCATION\")\n",
    "print(\"-\"*50)\n",
    "for i, asset in enumerate(assets):\n",
    "    print(f\"• {asset}: {recommended_weights[i]*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nEXPECTED PORTFOLIO METRICS\")\n",
    "print(\"-\"*50)\n",
    "print(f\"• Expected Annual Return: {recommended_return*100:.2f}%\")\n",
    "print(f\"• Expected Annual Volatility: {recommended_volatility*100:.2f}%\")\n",
    "print(f\"• Expected Sharpe Ratio: {recommended_sharpe:.3f}\")\n",
    "\n",
    "print(f\"\\nRISK CONSIDERATIONS\")\n",
    "print(\"-\"*50)\n",
    "print(\"• High allocation to TSLA increases portfolio volatility\")\n",
    "print(\"• Forecast uncertainty increases significantly over longer horizons\")\n",
    "print(\"• Model performance may vary under different market conditions\")\n",
    "print(\"• Regular rebalancing recommended to maintain optimal allocation\")\n",
    "\n",
    "print(f\"\\nIMPLEMENTATION RECOMMENDATIONS\")\n",
    "print(\"-\"*50)\n",
    "print(\"1. Implement recommended allocation with monthly rebalancing\")\n",
    "print(\"2. Monitor model performance and retrain quarterly\")\n",
    "print(\"3. Set stop-loss limits for individual positions\")\n",
    "print(\"4. Regular review of correlation assumptions\")\n",
    "print(\"5. Consider expanding universe to include more asset classes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF ANALYSIS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Model Diagnostics and Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional diagnostic plots and analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Diagnostics and Additional Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Residual analysis for ARIMA\n",
    "ax1 = axes[0, 0]\n",
    "residuals = arima_fitted.resid\n",
    "ax1.plot(residuals)\n",
    "ax1.set_title('ARIMA Model Residuals')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q plot for residuals\n",
    "ax2 = axes[0, 1]\n",
    "stats.probplot(residuals.dropna(), dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Residuals')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Asset correlation over time\n",
    "ax3 = axes[1, 0]\n",
    "rolling_corr = returns['TSLA'].rolling(60).corr(returns['SPY'])\n",
    "ax3.plot(rolling_corr.index, rolling_corr)\n",
    "ax3.set_title('TSLA-SPY Rolling Correlation (60-day)')\n",
    "ax3.set_ylabel('Correlation')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Volatility clustering\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(returns['TSLA'].index, returns['TSLA']**2)\n",
    "ax4.set_title('TSLA Squared Returns (Volatility Clustering)')\n",
    "ax4.set_ylabel('Squared Returns')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final model statistics\n",
    "print(\"\\nMODEL VALIDATION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ARIMA Model AIC: {arima_fitted.aic:.2f}\")\n",
    "print(f\"ARIMA Model BIC: {arima_fitted.bic:.2f}\")\n",
    "print(f\"Ljung-Box Test p-value: {arima_fitted.test_serial_correlation('ljungbox')[0]['lb_pvalue'].iloc[-1]:.4f}\")\n",
    "\n",
    "# Save key results for reporting\n",
    "results_summary = {\n",
    "    'best_model': best_model,\n",
    "    'forecast_return': total_return,\n",
    "    'optimal_weights': dict(zip(assets, recommended_weights)),\n",
    "    'portfolio_metrics': {\n",
    "        'return': recommended_return,\n",
    "        'volatility': recommended_volatility,\n",
    "        'sharpe': recommended_sharpe\n",
    "    },\n",
    "    'backtest_performance': strategy_outperformed\n",
    "}\n",
    "\n",
    "print(\"\\nAnalysis completed successfully!\")\n",
    "print(\"All results have been generated and are ready for reporting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
